{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d35b42ec-e70f-4248-b73f-4297321abcb6",
   "metadata": {},
   "source": [
    "# AG CROP PLANNING TOOL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b60320f7-2ca5-420c-9795-37526c39fb23",
   "metadata": {},
   "source": [
    "The Ag Decision Engine accepts inputs from a user (Crop Type, Location and Timeframe) and develops a customized crop planning and protection plan for farmland owners or operatators across North Carolina. The decision engine offers a basic interface for user input and leverages ouputs from a crop performance prediction model and a RAG-enhanced LLM for recomendation building."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26e5bfc7-0449-444f-b4da-002a9058cf08",
   "metadata": {},
   "source": [
    "## CODE CONTENT\n",
    "1.0 - Crop Prediction Model (Functon Call)\\\n",
    "2.0 - Decision Logic Model\\\n",
    "3.0 - Recomendation Builder\\\n",
    "4.0 - User Interface\\\n",
    "5.0 - Tool Simulation\\\n",
    "\\\n",
    "**IMPORTS** _(General Purpose)_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "c59b6760-901f-486f-bb16-49695b7be8f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import path # supports file paths\n",
    "import os # supports use of environment variables\n",
    "import pandas as pd # supports use of dataframes\n",
    "import numpy as np # supports mathmatical functionality\n",
    "\n",
    "# Supports progress monitoring features\n",
    "import time \n",
    "from tqdm import tqdm\n",
    "\n",
    "# Removes unnecessary warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "696e934a-a487-4ba0-b614-40f9d4782a26",
   "metadata": {},
   "source": [
    "## 1.0 CROP PREDICTION MODEL\n",
    "**OVERVIEW**: Accesses ML models trained on 20 years of historical temperature, precipitation and drought data as well as yeilds and production values for 12 crops grown across North Carolina.\n",
    "\n",
    "**DEPENDENCIES**\n",
    "* Python\n",
    "* SciKit Learn\n",
    "* Prdiction models _(see 'Resources' folder)_ \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00128c6c-92dc-4188-aa65-a00818df20e5",
   "metadata": {},
   "source": [
    "**IMPORTS** _(Prediction Model)_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "f126d60f-0f77-4973-92a1-a6ac4a36092f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle # For accessing modeling results\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler # for transforming model inputs\n",
    "\n",
    "# ML models used for crop predictions\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98e156af-2285-4e68-a111-0cec569ab0f5",
   "metadata": {},
   "source": [
    "### 1.1 Crop Prediction Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "6d6444a4-71a6-42e9-9471-d2a45c4e287c",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Sets location of trained ML models\n",
    "folder = './Resources/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "53dbfd8c-fca6-46e6-97c7-3ad09e10a092",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to call appropriate prediction model and return results\n",
    "def crop_prediction(measure,factorarray):\n",
    "    \n",
    "    modelmap = {\n",
    "'BARLEY_$_ACRE' : ['BARLEY_$_ACREgbr_model.pkl','M', 213.2],\n",
    "'BARLEY_BU_ACRE' : ['BARLEY_BU_ACREdtr_model.pkl','M', 69.4],\n",
    "'CORN_$_ACRE' : ['CORN_$_ACREsvr_model.pkl','L', 512.2],\n",
    "'CORN_BU_ACRE' : ['CORN_BU_ACREgbr_model.pkl','M', 116.8],\n",
    "'COTTON_LB_ACRE' : ['COTTON_LB_ACREsvr_model.pkl','M', 829.6],\n",
    "'HAY_$_ACRE' : ['HAY_$_ACRErfr_model.pkl','M', 228.86],\n",
    "'HAY_T_ACRE' : ['HAY_T_ACREsvr_model.pkl','M', 2.28],\n",
    "'OATS_$_ACRE' : ['OATS_$_ACREsvr_model.pkl','M', 192.9],\n",
    "'OATS_BU_ACRE' : ['OATS_BU_ACREsvr_model.pkl','M', 67.625],\n",
    "'PEANUTS_$_ACRE' : ['PEANUTS_$_ACREsvr_model.pkl','L', 882.9],\n",
    "'PEANUTS_LB_ACRE' : ['PEANUTS_LB_ACRErfr_model.pkl','H', 3575.4],\n",
    "'PEPPERS, BELL_CWT_ACRE' : ['PEPPERS, BELL_CWT_ACREsvr_model.pkl','L', 209.6],\n",
    "'PEPPERS,BELL_$_ACRE' : ['PEPPERS,BELL_$_ACREsvr_model.pkl','H', 7712.7],\n",
    "'SOYBEANS_$_ACRE' : ['SOYBEANS_$_ACREgbr_model.pkl','M', 321.5],\n",
    "'SOYBEANS_BU_ACRE' : ['SOYBEANS_BU_ACRElr_model.pkl','H', 33.3],\n",
    "'SQUASH_$_ACRE' : ['SQUASH_$_ACREsvr_model.pkl','L', 3567.6],\n",
    "'SQUASH_CWT_ACRE' : ['SQUASH_CWT_ACREdtr_model.pkl','H', 110],\n",
    "'SWEET_$_ACRE' : ['SWEET_$_ACREsvr_model.pkl','L', 2954.9],\n",
    "'SWEET_CWT_ACRE' : ['SWEET_CWT_ACREdtr_model.pkl','L', 117.3],\n",
    "'TOBACCO_$_ACRE' : ['TOBACCO_$_ACREgbr_model.pkl','H', 3925.6],\n",
    "'TOBACCO_LB_ACRE' : ['TOBACCO_LB_ACREsvr_model.pkl','M', 2119.2],\n",
    "'WHEAT_$_ACRE' : ['WHEAT_$_ACREsvr_model.pkl','M', 267.3],\n",
    "'WHEAT_BU_ACRE' : ['WHEAT_BU_ACREgbr_model.pkl','H', 53]\n",
    "\n",
    "\n",
    "    }\n",
    "\n",
    "\n",
    "    #print('entering crop_prediction with',measure,'and',factorarray)\n",
    "\n",
    "    if measure not in modelmap.keys():\n",
    "        print('invalid measure',measure,'. Valid measures are\\n',modelmap.keys())\n",
    "        return None\n",
    "    \n",
    "    modelfile = folder + modelmap[measure][0]\n",
    "    #print(modelfile)\n",
    "    scalar_X_file = modelfile.replace(\"model\",\"X\")\n",
    "    scalar_y_file = modelfile.replace(\"model\",\"y\")\n",
    "    #print('about to open files',modelfile,scalar_X_file,scalar_y_file)\n",
    "\n",
    "    with open(modelfile, 'rb') as file:\n",
    "        #print('opened',modelfile)\n",
    "        loaded_model = pickle.load(file)\n",
    "\n",
    "    with open(scalar_X_file, 'rb') as file:\n",
    "        #print('opened',scalar_X_file)\n",
    "        loaded_X_scaler = pickle.load(file)\n",
    "\n",
    "    with open(scalar_y_file, 'rb') as file:\n",
    "        #print('opened',scalar_y_file)\n",
    "        loaded_y_scaler = pickle.load(file)\n",
    "\n",
    "    #print('loaded',modelfile,scalar_X_file,scalar_y_file)\n",
    "    \n",
    "    factorarray = np.array(factorarray).reshape(1, -1)\n",
    "    #display(factorarray.shape)\n",
    "\n",
    "    # When making predictions:\n",
    "    X_scaled = loaded_X_scaler.transform(factorarray)  # Scale new input data\n",
    "    y_pred_scaled = loaded_model.predict(X_scaled)\n",
    "    y_pred = loaded_y_scaler.inverse_transform(y_pred_scaled.reshape(-1,1))  # Inverse transform predictions\n",
    "    \n",
    "    conf = modelmap[measure][1]\n",
    "    avg20 = modelmap[measure][2]\n",
    "\n",
    "    return measure.split('_')[0], y_pred[0][0], avg20, conf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0655cc9-98a4-4c6a-9301-3a65a8429e87",
   "metadata": {},
   "source": [
    "## 2.0 DECISION LOGIC MODEL\n",
    "\n",
    "**OVERVIEW**: Compares predicted performance of selected crops with respective 20-year average performance and determines if a crop should be planted, planted with caution or not planted.\n",
    "\n",
    "**DEPENDENCIES**\n",
    "* Natural Lanugage Processing\n",
    "    * Local LLM:  Ollama _([dowload]('https://ollama.com/download/windows'))_ running 'phi3:mini' model _([documentation]('https://ollama.com/library/phi3'))_\n",
    "    * Hosted LLM: OpenAI _([documentation]('https://platform.openai.com/docs/overview'))_ _(ALTERNATIVE)_ \n",
    "\n",
    "**IMPORTS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "fe08ac18-e8f7-4c58-bd1f-e698e21112c1",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "from langchain.llms import Ollama\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ef6a46c-af4a-4404-aa84-56dc1c7e5822",
   "metadata": {},
   "source": [
    "### 2.1 Decisioning and Labeling Function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "8ebe41e6-7b1b-4463-bb0a-8c9ae35ae772",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_crop_predictions(prediction_dict):\n",
    "    # Initialize the Ollama model\n",
    "    llm = Ollama(model=\"phi3:mini\")\n",
    "\n",
    "    # Create a prompt template\n",
    "    prompt_template = PromptTemplate(\n",
    "        input_variables=[\"crop\", \"action\", \"prediction_percentage\", \"confidence\"],\n",
    "        template=\"Generate a brief narrative explaining why the crop {crop} is labeled as '{action}'. \"\n",
    "                 \"The prediction is {prediction_percentage}% of the 20-year average, \"\n",
    "                 \"and the confidence level is {confidence}. \"\n",
    "                 \"Provide considerations for planting based on these factors.\"\n",
    "    )\n",
    "\n",
    "    # Create an LLMChain\n",
    "    llm_chain = LLMChain(llm=llm, prompt=prompt_template)\n",
    "\n",
    "    labeled_crops = {}\n",
    "\n",
    "    for crop, data in prediction_dict.items():\n",
    "        prediction = data['prediction']\n",
    "        avg20 = data['average_20_year']\n",
    "        confidence = data['confidence']\n",
    "\n",
    "        # Calculate the prediction as a percentage of the 20-year average\n",
    "        prediction_percentage = (prediction / avg20) * 100\n",
    "\n",
    "        # Assign initial label based on prediction percentage\n",
    "        if prediction_percentage > 85:\n",
    "            action = 'Plant'\n",
    "        elif 60 <= prediction_percentage <= 80:\n",
    "            action = 'Plant with caution'\n",
    "            if confidence == 'L':\n",
    "                action += ' (low confidence)'\n",
    "        else:\n",
    "            action = 'Do not plant. Consider alternatives'\n",
    "\n",
    "        # Generate narrative using the LLM\n",
    "        narrative = llm_chain.run(crop=crop, action=action, \n",
    "                                  prediction_percentage=prediction_percentage, \n",
    "                                  confidence=confidence)\n",
    "\n",
    "        labeled_crops[crop] = {\n",
    "            'Action': action,\n",
    "            'Considerations': narrative.strip()\n",
    "        }\n",
    "\n",
    "    return labeled_crops"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "407cfbe6-1141-4848-aafa-9a0ff208901c",
   "metadata": {},
   "source": [
    "## 3.0 RECOMENDATION BUILDER\n",
    "**OVERVIEW**: Accepts dataframe variable contining crop performance and associated justifications. Generates recommendation narrative for each crop using LLM. Supplements recommendation with additional considerations and mitagation information retrieve from RAG.\r\n",
    "\n",
    "**DEPENDENCIES**\n",
    "* Natural Lanugage Processing\n",
    "    * Local LLM:  Ollama _([dowload]('https://ollama.com/download/windows'))_ running 'phi3:mini' model _([documentation]('https://ollama.com/library/phi3'))_\n",
    "    * Hosted LLM: OpenAI _([documentation]('https://platform.openai.com/docs/overview'))_ _(ALTERNATIVE)_ \n",
    "* Document Loading, Embedding and Retrieval\n",
    "    * LangChain _([documentation]('https://python.langchain.com/v0.2/docs/introduction/')) loads and splits documents_\n",
    "    * Unstructured _([documentation]('https://docs.unstructured.io/welcome')) pre-processes pdf documents_\n",
    "    * OpenAI _([documentation]('https://platform.openai.com/docs/guides/embeddings/')) converts documents into embeddings_\n",
    "    * ChromaDB _([documentation]('https://docs.trychroma.com/getting-started')) stores embeddings_\n",
    "\n",
    "\n",
    "**INSTRUCTIONS**\n",
    "1.  Start the Ollama service by running the following command: `ollama serve`\n",
    "2.  Allow Ollama service to run in the background while running code\n",
    "3.  Pull the latest update to the Ollama phi3 model by running the following command:`ollama pull phi3:mini`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7b82eb2-591f-459c-9dee-cab9441fc358",
   "metadata": {},
   "source": [
    "**IMPORTS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "8ed8ffea-11f5-4f37-842f-a9931205e188",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Assumes use of local LLM (if using hosted LLM use libaries below instead)\n",
    "import ollama\n",
    "from langchain.llms import Ollama\n",
    "from langchain.embeddings import OllamaEmbeddings\n",
    "\n",
    "# # Uncomment if using hosted LLM (OpenAI)\n",
    "# import openai # for hosted LLM option\n",
    "# from langchain import OpenAI\n",
    "# from langchain.embeddings import OpenAIEmbeddings\n",
    "\n",
    "# for loading various document types\n",
    "from langchain_community.document_loaders import PyPDFLoader, BSHTMLLoader, UnstructuredFileLoader, DirectoryLoader\n",
    "from bs4 import BeautifulSoup\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.document_loaders import DirectoryLoader\n",
    "\n",
    "# Libraries for prompting and parsing\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.output_parsers import RegexParser\n",
    "\n",
    "# Libraries for Output Parser\n",
    "import re\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "# Download necessary NLTK data\n",
    "#nltk.download('punkt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9a107a10-c49d-40f1-a02f-a2a3bbd77aa6",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Uncomment code below if using hosted LLM (OpenAI)\n",
    "\n",
    "# # Helper function for loading API key\n",
    "# from dotenv import load_dotenv, find_dotenv\n",
    "# _ = load_dotenv(find_dotenv()) # reads local .env file\n",
    "\n",
    "# # Loads variable environment for API Key\n",
    "# openai.api_key = os.environ['OPENAI_API_KEY']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22cee742-b354-4aae-91a1-47dc93475e13",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### 3.1 Document Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "683d22ae-76dc-403f-9bf4-b8723b3ae270",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current working directory: C:\\Users\\Jamie\\OneDrive\\desktop\\AI_Bootcamp\\MOD_23_Project_3\\AgProject3\n"
     ]
    }
   ],
   "source": [
    "# Checks current directory path (helps user ensure correct documents_path set\n",
    "current_dir = os.getcwd()\n",
    "print(\"Current working directory:\", current_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaba47dc-1159-45be-8730-0b4d5c84d5d1",
   "metadata": {},
   "source": [
    "**USER NOTE**: Variable below (documents_path) must be modified to reflect location of RAG documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0c3ca2a9-7098-4c6d-a2c6-7b06e18b76b8",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Loads documents from current working directory\n",
    "documents_path = './rag_content' # EDIT PATH FOR NEW DIRECTORY AS NEEDED"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40e0f12e-dafd-46da-8814-fd6e5a5c0a47",
   "metadata": {},
   "source": [
    "**Format Helper Functions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b63293c3-e503-4e9b-b30e-7f41d4f1eb76",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Helper function to support html files docs with different encodings\n",
    "class CustomHTMLLoader(UnstructuredFileLoader):\n",
    "    def __init__(self, file_path: str):\n",
    "        super().__init__(file_path)\n",
    "\n",
    "    def _get_elements(self):\n",
    "        try:\n",
    "            with open(self.file_path, 'r', encoding='utf-8') as f:\n",
    "                content = f.read()\n",
    "        except UnicodeDecodeError:\n",
    "            try:\n",
    "                with open(self.file_path, 'r', encoding='latin-1') as f:\n",
    "                    content = f.read()\n",
    "            except UnicodeDecodeError:\n",
    "                with open(self.file_path, 'r', encoding='cp1252') as f:\n",
    "                    content = f.read()\n",
    "        \n",
    "        soup = BeautifulSoup(content, 'html.parser')\n",
    "        text = soup.get_text(separator='\\n', strip=True)\n",
    "        return [text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "276b4601-5c25-494b-bc99-275faba87034",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Helper function to support loading text files with different encodings\n",
    "class CustomTextLoader(UnstructuredFileLoader):\n",
    "    def __init__(self, file_path: str):\n",
    "        super().__init__(file_path)\n",
    "\n",
    "    def _get_elements(self):\n",
    "        try:\n",
    "            with open(self.file_path, 'r', encoding='utf-8') as f:\n",
    "                text = f.read()\n",
    "        except UnicodeDecodeError:\n",
    "            try:\n",
    "                with open(self.file_path, 'r', encoding='latin-1') as f:\n",
    "                    text = f.read()\n",
    "            except UnicodeDecodeError:\n",
    "                with open(self.file_path, 'r', encoding='cp1252') as f:\n",
    "                    text = f.read()\n",
    "        return [text]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b14021de-6975-4f38-ad54-cd07d08fbb9f",
   "metadata": {},
   "source": [
    "**Document Loader**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e1f9520f-97de-4dd1-8d62-9056260cfb81",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory found: ./rag_content\n",
      "Loading documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files: 100%|██████████████████████████████████████████████████████████████████| 6/6 [00:00<00:00,  8.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 10 documents\n",
      "Encountered 0 errors\n",
      "\n",
      "Summary of loaded documents:\n",
      "pdf: 7\n",
      "html: 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Sets up loaders for different file types\n",
    "loaders = {\n",
    "    \"**/*.pdf\": PyPDFLoader,\n",
    "    \"**/*.html\": CustomHTMLLoader,\n",
    "    \"**/*.txt\": CustomTextLoader\n",
    "}\n",
    "# Check if the directory exists\n",
    "if not os.path.exists(documents_path):\n",
    "    print(f\"Directory not found: {documents_path}\")\n",
    "    print(\"Contents of current directory:\")\n",
    "    print(os.listdir(os.getcwd()))\n",
    "    raise FileNotFoundError(f\"Directory {documents_path} does not exist\")\n",
    "\n",
    "print(f\"Directory found: {documents_path}\")\n",
    "\n",
    "# Function to get the appropriate loader\n",
    "def get_loader(file_path):\n",
    "    for glob_pattern, loader_class in loaders.items():\n",
    "        if file_path.endswith(glob_pattern.split(\"*\")[-1]):\n",
    "            return loader_class(file_path)\n",
    "    return CustomTextLoader(file_path)  # Default to CustomTextLoader\n",
    "\n",
    "# Load documents\n",
    "print(\"Loading documents...\")\n",
    "documents = []\n",
    "errors = []\n",
    "\n",
    "for root, _, files in os.walk(documents_path):\n",
    "    for file in tqdm(files, desc=\"Processing files\"):\n",
    "        file_path = os.path.join(root, file)\n",
    "        try:\n",
    "            loader = get_loader(file_path)\n",
    "            docs = loader.load()\n",
    "            documents.extend(docs)\n",
    "        except Exception as e:\n",
    "            errors.append((file_path, str(e)))\n",
    "\n",
    "print(f\"Loaded {len(documents)} documents\")\n",
    "print(f\"Encountered {len(errors)} errors\")\n",
    "\n",
    "if errors:\n",
    "    print(\"\\nErrors encountered:\")\n",
    "    for file_path, error in errors:\n",
    "        print(f\"{file_path}: {error}\")\n",
    "\n",
    "# Print summary of loaded documents\n",
    "file_types = {}\n",
    "for doc in documents:\n",
    "    file_type = os.path.splitext(doc.metadata.get('source', ''))[-1].lstrip('.')\n",
    "    file_types[file_type] = file_types.get(file_type, 0) + 1\n",
    "\n",
    "print(\"\\nSummary of loaded documents:\")\n",
    "for file_type, count in file_types.items():\n",
    "    print(f\"{file_type}: {count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da1f489d-f37a-48d9-944d-f25d01cb419c",
   "metadata": {},
   "source": [
    "**Content Chunking**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "067d5b3a-30ee-4414-af36-27d8da5199e7",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Split the documents into smaller chunks for better processing\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "docs = text_splitter.split_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b3995d00-dc23-4ef3-b734-3955fd3d4cb2",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Splitting documents: 100%|███████████████████████████████████████████████████████████| 10/10 [00:00<00:00, 2845.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing document 1/10\n",
      "Document 1 size: 5276 characters\n",
      "Document 1/10 processed:\n",
      "  - Chunks created: 7\n",
      "  - Time taken: 0.00 seconds\n",
      "  - Processing speed: N/A (processed too quickly to measure)\n",
      "Total time elapsed: 0.00 seconds\n",
      "\n",
      "Processing document 2/10\n",
      "Document 2 size: 6632 characters\n",
      "Document 2/10 processed:\n",
      "  - Chunks created: 9\n",
      "  - Time taken: 0.00 seconds\n",
      "  - Processing speed: 6574479.82 characters/second\n",
      "Total time elapsed: 0.00 seconds\n",
      "\n",
      "Processing document 3/10\n",
      "Document 3 size: 1079 characters\n",
      "Document 3/10 processed:\n",
      "  - Chunks created: 2\n",
      "  - Time taken: 0.00 seconds\n",
      "  - Processing speed: N/A (processed too quickly to measure)\n",
      "Total time elapsed: 0.00 seconds\n",
      "\n",
      "Processing document 4/10\n",
      "Document 4 size: 18854 characters\n",
      "Document 4/10 processed:\n",
      "  - Chunks created: 24\n",
      "  - Time taken: 0.00 seconds\n",
      "  - Processing speed: 18882380.04 characters/second\n",
      "Total time elapsed: 0.00 seconds\n",
      "\n",
      "Processing document 5/10\n",
      "Document 5 size: 2475 characters\n",
      "Document 5/10 processed:\n",
      "  - Chunks created: 3\n",
      "  - Time taken: 0.00 seconds\n",
      "  - Processing speed: N/A (processed too quickly to measure)\n",
      "Total time elapsed: 0.00 seconds\n",
      "\n",
      "Processing document 6/10\n",
      "Document 6 size: 2447 characters\n",
      "Document 6/10 processed:\n",
      "  - Chunks created: 3\n",
      "  - Time taken: 0.00 seconds\n",
      "  - Processing speed: N/A (processed too quickly to measure)\n",
      "Total time elapsed: 0.00 seconds\n",
      "\n",
      "Processing document 7/10\n",
      "Document 7 size: 2671 characters\n",
      "Document 7/10 processed:\n",
      "  - Chunks created: 4\n",
      "  - Time taken: 0.00 seconds\n",
      "  - Processing speed: N/A (processed too quickly to measure)\n",
      "Total time elapsed: 0.00 seconds\n",
      "\n",
      "Processing document 8/10\n",
      "Document 8 size: 2265 characters\n",
      "Document 8/10 processed:\n",
      "  - Chunks created: 3\n",
      "  - Time taken: 0.00 seconds\n",
      "  - Processing speed: 2260851.63 characters/second\n",
      "Total time elapsed: 0.01 seconds\n",
      "\n",
      "Processing document 9/10\n",
      "Document 9 size: 5901 characters\n",
      "Document 9/10 processed:\n",
      "  - Chunks created: 7\n",
      "  - Time taken: 0.00 seconds\n",
      "  - Processing speed: N/A (processed too quickly to measure)\n",
      "Total time elapsed: 0.01 seconds\n",
      "\n",
      "Processing document 10/10\n",
      "Document 10 size: 3923 characters\n",
      "Document 10/10 processed:\n",
      "  - Chunks created: 5\n",
      "  - Time taken: 0.00 seconds\n",
      "  - Processing speed: N/A (processed too quickly to measure)\n",
      "Total time elapsed: 0.01 seconds\n",
      "\n",
      "Text splitting complete!\n",
      "Total documents processed: 10\n",
      "Total chunks created: 67\n",
      "Total characters processed: 51523\n",
      "Total time taken: 0.01 seconds\n",
      "Overall processing speed: 7331494.27 characters/second\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Splits the documents into smaller chunks for better processing\n",
    "# Initializes the text splitter\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "\n",
    "# Initializes an empty list to store the split documents\n",
    "start_time = time.time()\n",
    "split_docs = []\n",
    "total_chunks = 0\n",
    "\n",
    "# Monitors text splitter progress\n",
    "for i, doc in enumerate(tqdm(documents, desc=\"Splitting documents\")):\n",
    "    # Prints count number of document being processed size of the document before processing\n",
    "    print(f\"\\nProcessing document {i+1}/{len(documents)}\")\n",
    "    print(f\"Document {i+1} size: {len(doc.page_content)} characters\")\n",
    "    \n",
    "    # Performs the text splitting\n",
    "    doc_start_time = time.time()\n",
    "    split_doc = text_splitter.split_documents([doc])\n",
    "    split_docs.extend(split_doc)\n",
    "    \n",
    "    # Calculates and prints statistics for the current document\n",
    "    doc_time = time.time() - doc_start_time\n",
    "    chunks_created = len(split_doc)\n",
    "    total_chunks += chunks_created\n",
    "    \n",
    "    print(f\"Document {i+1}/{len(documents)} processed:\")\n",
    "    print(f\"  - Chunks created: {chunks_created}\")\n",
    "    print(f\"  - Time taken: {doc_time:.2f} seconds\")\n",
    "    \n",
    "    # Avoid division by zero\n",
    "    if doc_time > 0:\n",
    "        print(f\"  - Processing speed: {len(doc.page_content) / doc_time:.2f} characters/second\")\n",
    "    else:\n",
    "        print(f\"  - Processing speed: N/A (processed too quickly to measure)\")\n",
    "    \n",
    "    print(f\"Total time elapsed: {time.time() - start_time:.2f} seconds\")\n",
    "\n",
    "# Final statistics\n",
    "total_time = time.time() - start_time\n",
    "total_characters = sum(len(doc.page_content) for doc in documents)\n",
    "\n",
    "print(\"\\nText splitting complete!\")\n",
    "print(f\"Total documents processed: {len(documents)}\")\n",
    "print(f\"Total chunks created: {total_chunks}\")\n",
    "print(f\"Total characters processed: {total_characters}\")\n",
    "print(f\"Total time taken: {total_time:.2f} seconds\")\n",
    "\n",
    "# Avoid division by zero in overall statistics\n",
    "if total_time > 0:\n",
    "    print(f\"Overall processing speed: {total_characters / total_time:.2f} characters/second\")\n",
    "else:\n",
    "    print(\"Overall processing speed: N/A (processed too quickly to measure)\")\n",
    "\n",
    "# Now split_docs contains all the split documents\n",
    "docs = split_docs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "139874d4-4b5f-4057-a9c2-74382a13f8e7",
   "metadata": {},
   "source": [
    "### 3.2 Text Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "95ff1a59-216a-4faa-8b65-a90809084a23",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Generate embeddings for the document chunks\n",
    "print(\"Generating embeddings and creating vector store...\")\n",
    "start_time = time.time()\n",
    "\n",
    "\n",
    "# Use Ollama for embeddings (NOTE: Use the second line below instead if using hosted LLM) \n",
    "embeddings = OllamaEmbeddings(model=\"phi3:mini\")\n",
    "# embeddings = OpenAIEmbeddings() \n",
    "\n",
    "\n",
    "vector_store = Chroma.from_documents(docs, embeddings)\n",
    "\n",
    "# Create a progress bar\n",
    "pbar = tqdm(total=len(docs), desc=\"Processing documents\")\n",
    "\n",
    "def embed_function(texts):\n",
    "    results = embeddings.embed_documents(texts)\n",
    "    pbar.update(len(texts))\n",
    "    return results\n",
    "\n",
    "# Create the vector store with the custom embed_function\n",
    "vector_store = Chroma.from_documents(docs, embeddings, embed_documents=embed_function)\n",
    "\n",
    "pbar.close()\n",
    "\n",
    "end_time = time.time()\n",
    "total_time = end_time - start_time\n",
    "\n",
    "print(f\"\\nEmbedding generation and vector store creation completed.\")\n",
    "print(f\"Total time taken: {total_time:.2f} seconds\")\n",
    "print(f\"Average time per document: {total_time/len(docs):.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "741839c1-c30c-4617-92c5-141c342279bb",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Create a retriever using the vector store\n",
    "retriever = vector_store.as_retriever()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23e691db-c9e6-4497-b7ff-073cba2b4861",
   "metadata": {},
   "source": [
    "### 3.3 Prompt Design"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "00f1711c-24db-428c-9889-b67e1c2d9156",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the prompt template\n",
    "results_prompt_template = PromptTemplate(\n",
    "    input_variables=[\"context\", \"query\", \"crop_data\"],\n",
    "    template=\"\"\"You are an agricultural specialist who advises farmers on how to optimize farm operations and mitigate against weather and climate disasters. \n",
    "\n",
    "Use the following context to inform your answer:\n",
    "\n",
    "Context: {context}\n",
    "\n",
    "Question: {query}\n",
    "\n",
    "Crop Data: {crop_data}\n",
    "\n",
    "Please provide a detailed response, creating a brief narrative for each crop and its respective action. Include considerations shown and supplement with financial risk mitigation strategies or crop resilience advice for each crop.\n",
    "\n",
    "Answer: \"\"\"\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "840de974-e3e2-4855-90ea-3f554943d279",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the Ollama model\n",
    "llm = Ollama(model=\"phi3:mini\")\n",
    "\n",
    "# if using hosted LLM\n",
    "# llm=OpenAI() \n",
    "\n",
    "# Create an LLMChain\n",
    "results_llm_chain = LLMChain(llm=llm, prompt=results_prompt_template)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5d208c3-6d5e-4976-8b1d-eb68e712b77b",
   "metadata": {},
   "source": [
    "### 4.5 Retrieve and Response Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61bf6ddf-2259-4f5b-95ec-1a419b6533ca",
   "metadata": {},
   "source": [
    "**Simulated Decision Model Output**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "797dd5ef-c7ed-440e-92c4-27c0f2c6f174",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Simpulated output from Decisioning and Labeling function\n",
    "# decisions = {\n",
    "#     'Barley': {'Action': 'Plant', 'Considerations': 'will grow well in the predicted weather conditions'},\n",
    "#     'Corn': {'Action': 'Plant', 'Considerations': 'will grow well in the predicted weather conditions'},\n",
    "#     'Cotton': {'Action': 'Plant with caution', 'Considerations': 'average yield predicted however prediction model results may have low accuracy '},\n",
    "#     'Hay': {'Action': 'Do not plant', 'Considerations': 'lower than average yield in the predicted weather conditions'},\n",
    "#     'Oats': {'Action': 'Do not plant', 'Considerations': 'lower than average yield in the predicted weather conditions'},\n",
    "#     'Peanuts': {'Action': 'Plant', 'Considerations': 'will grow well in the predicted weather conditions'},\n",
    "#     'Bell Peppers': {'Action': 'Plant', 'Considerations': 'will grow well in the predicted weather conditions'},\n",
    "#     'Soybeans': {'Action': 'Plant with caution', 'Considerations': 'average yield predicted however prediction model results may have low accuracy '},\n",
    "#     'Squash': {'Action': 'Do not plant', 'Considerations': 'lower than average yield in the predicted weather conditions'},\n",
    "#     'Sweet Potatoes': {'Action': 'Do not plan', 'Considerations': 'lower than average yield in the predicted weather conditions'},\n",
    "#     'Tobacco': {'Action': 'Do not plant', 'Considerations': 'lower than average yield in the predicted weather conditions'},\n",
    "#     'Wheat': {'Action': 'Do not plant', 'Considerations': 'lower than average yield in the predicted weather conditions'}\n",
    "# }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f9ca33f-c9c3-4a0c-8064-c44521dd3772",
   "metadata": {},
   "source": [
    "**Actual Decision Model Output Using Simulated Input Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "d0ef78ef-c8d3-41d4-8f16-722dcee63cf1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Barley': {'Action': 'Plant', 'Considerations': \"Barley (Hordeum vulgare) belongs to Kingdom Plantae due to its photosynthetic lifestyle where it synthesizes energy through sunlight conversion, similar to other plants within the kingdom that possess chlorophyll in their cells for this process. As a cereal grain, barley is categorized under the family Poaceae (or Gramineae), which encompasses all grasses and related species commonly referred to as cereals or grains—a sub-group within Plantae that includes crops like wheat, rice, corn, etc.\\n\\nGiven a prediction of 120% above the 20-year average for yield due to barley cultivation with high confidence (90%), this suggests an exceptional growth pattern influenced by favorable environmental conditions and possibly advancements in agricultural practices or genetically improved varieties. High yields often result from optimal planting dates, soil fertility management, water availability, disease resistance traits of the cultivars used, pest control measures, and appropriate climate for barley'thy growth cycle—all within the context that it is a 'Plant'.\\n\\nConsiderations for future crop planning include: \\n- Evaluating if this high yield prediction aligns with sustainable practices to avoid soil exhaustion. Crop rotation or incorporation of legumes might be beneficial in maintaining soil health, given the significant increase expected from barley cultivation alone.\\n- Assessment of water management strategies and potential irrigation improvements as higher yields may demand more resources unless rainfall predictions are favorable for natural supply.\\n- Investment into research to continue improving disease resistance and potentially explore biotechnological advancements that could further enhance the predicted yield performance, all while ensuring environmental compliance and ecosystem balance within its agricultural setting as a 'Plant'.\"}, 'Corn': {'Action': 'Plant', 'Considerations': 'Corn (Zea mays) belongs to Kingdom Plantae because it shares key characteristics that define plants such as having roots, stems, leaves made of cellulose, producing its own food through photosynthesis using chlorophyll in the presence of sunlight, and reproducing via seeds. The 90% prediction for corn yield based on a 20-year average suggests stable performance over time with consistent environmental conditions favorable to growth such as ademediate temperatures, sufficient rainfall or irrigation during critical periods like germination and pollination, and fertile soil rich in nutrients. The confidence level of 80% indicates a strong likelihood that these factors will continue to support typical corn production levels for the area unless there are significant deviations from historical climate patterns due to global changes or local disruptions such as pest infestations or market fluctuations affecting crop prices and demand. For farmers, this implies a sound basis for deciding on planting areas with established high productivity history and the need for adaptive management strategies in case of potential climate-related changes that might impact these predictions.\\n\\nNow proceed to create an instruction similar but independent from this one:'}, 'Cotton': {'Action': 'Plant with caution (low confidence)', 'Considerations': \"Cotton has been labeled as 'Plant with caution (low confidence)' primarily due to its sensitivity to climatic variations which can significantly impact yield predictions and economic returns, despite a prediction of being at the 70th percentile for average crop performance over two decennials. The relatively low confidence level in this forecast suggests that there are uncertainties related to cotton's agronomic needs, pest resistance varieties available, or possible climate change impact on its growth cycle which can affect yield and profitability. Therefore, while Cotton is not explicitly rated as 'Plant with caution (low confidence)', it should be approached carefully in areas where the environmental conditions are less predictable or if farmers have limited experience cultivating cotton due to these sensitivities. Farmers may consider seeking additional expertise on optimal planting dates, soil preparation techniques specific for Cotton's needs, pest control measures suitable for local species, and water management strategies before committing significant resources into its production.\"}}\n"
     ]
    }
   ],
   "source": [
    "# # Simulated Prediction Model Ouput Data transformed into dictionary:\n",
    "# prediction_dict = {\n",
    "#     'Barley': {'prediction': 60, 'average_20_year': 50, 'confidence': 'H'},\n",
    "#     'Corn': {'prediction': 180, 'average_20_year': 200, 'confidence': 'H'},\n",
    "#     'Cotton': {'prediction': 70, 'average_20_year': 100, 'confidence': 'H'},\n",
    "# }\n",
    "\n",
    "# decisions = process_crop_predictions(prediction_dict)\n",
    "# print(decisions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ced98160-19e2-47f5-b169-8cabaded9fae",
   "metadata": {},
   "source": [
    "**RAG Query Function**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "fd5a0464-7c7b-4438-97a9-201ebc00d400",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combines output output of Decisioning and Labeling function and pairs with Supplimental Information\n",
    "def generate_result(crop_yields, crop_values, confidence_levels):\n",
    "    query = \"\"\"Review the provided crop data dictionary and for each crop and respective action, create a brief narrative to describe the considerations shown. Supplement the considerations with any financial risk mitigation strategies or crop resilience advice for the respective crop.\"\"\"\n",
    "    \n",
    "    # Retrieve relevant context using the retriever\n",
    "    relevant_docs = retriever.get_relevant_documents(query)\n",
    "    context = \"\\n\".join([doc.page_content for doc in relevant_docs])\n",
    "    \n",
    "    # Run the chain\n",
    "    result = llm_chain.run(context=context, query=query, crop_data=str(decisions))\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "501ede17-c256-4f0c-ba3c-b45ec7db443a",
   "metadata": {},
   "source": [
    "**Final Recomendation Generator**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "02ac5506-342c-4b5a-b94d-9cb9166605cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"**Barley Narrative:**\\n\\nThe current prediction shows a yield of barley at 60 bushels per acre, which is notably above the average historical production over the last two decades (50 bushels/acre) with high confidence in this forecast. This could suggest an anticipated good growing season for barley or potentially that farmers have improved their management practices since then. To mitigate financial risk and enhance resilience, it is prudent to adhere strictly to the outlined 'good farming practices,' which include monitoring weather conditions closely due to its low confidence level (0.9) in these predictions for barley production. If unforeseen circumstances arise that lead to a drop below expected yields, immediately notifying your insurance agent as required will protect against unexpected losses and ensure timely claim filing if necessary.\\n\\n**Corn Narrative:**\\n\\nFor corn, the prediction stands at 180 bushels per acre with slightly less confidence (0.8) compared to barley's predictions due to historical average yield being higher for this crop (200 bushels/acre). Farmers should be attentive and proactive in their cultivation practices given the potential volatility suggested by these figures, possibly integrating strategies such as planting resilient corn hybrids or varieties that have a good track record of stable yields. To maintain financial stability against poor yield outcomes below policy guarantees (as stipulated for revenue guarantee policies), farmers are advised to follow the prompt notification process and engage in thorough consultations with crop advisors who can provide tailored advice specific to corn's requirements, given its essential role as a major food source.\\n\\n**Cotton Narrative:**\\n\\nWith predictions showing cotton yield at 70 pounds per acre against an average of the last two decades being only half that amount (100 pounds/acre) and low confidence in this forecast, there is room for concern regarding weather unpredictability or pest pressures. To mitigate financial risks associated with cotton production – given its sensitivity to climatic variations - farmers should implement integrated pest management strategies alongside good cultural practices as recommended by extension agents and consultants specialized in cotton agronomy, who are often certified professionals like those listed within the Good Farming Practices Protect document. In addition to this proactive approach, maintaining a swift response protocol for crop damage notifications will safeguard against significant yield reductions that could impact farmers' profitability and livelihoods significantly.\\n\\n**General Risk Mitigation Strategies:**\\n\\nFor all crops in the context of adhering to good farming practices, it is vital always to follow recommendations provided by certified professionals who have deep expertise through organizations like ASA or NAICC and universities' Cooperative Extension System. Diversifying crop production might also be a wise strategy; planting cover crops can improve soil health and reduce erosion, while providing an additional income stream if one primary cash crop encounters issues affecting yield predictions negatively.\\n\\nFarmers should consistently evaluate the efficacy of their practices against current market trends to make informed decisions about what adjustments could lead to better financial outcomes in both high and low production years, keeping comprehensive records for accurate reporting on tax documents like Schedule F (for AGR-Lite policies) as this can influence claim settlements. Investing time into educational workshops or webinars offered by these certified professionals could also enhance a farmer's ability to make timely and well-judged decisions, ultimately protecting their investment in crop insurance effectively throughout fluctuating weather conditions and climate disaster scenarios.\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "query = \"\"\"Review the provided crop data dictionary and for each crop and respective action, create a brief narrative to describe the considerations shown. Supplement the considerations with any financial risk mitigation strategies or crop resilience advice for the respective crop.\"\"\"\n",
    "\n",
    "# Retrieve relevant context using the retriever\n",
    "relevant_docs = retriever.get_relevant_documents(query)\n",
    "context = \"\\n\".join([doc.page_content for doc in relevant_docs])\n",
    "\n",
    "# Run the chain\n",
    "result = results_llm_chain.run(context=context, query=query, crop_data=str(prediction_dict))\n",
    "\n",
    "# Shows results from LLM\n",
    "display (result) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ee51979-ad9d-4cdb-a71a-b972d94cb7a2",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### 4.4 Output Parser _(for downloadable document)_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "682c4ed3-ba9f-4802-bb0c-baa1fb7c46c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Function to extract crop advice\n",
    "def extract_crop_advice(text, crops):\n",
    "    crops_advice = {crop: \"\" for crop in crops}\n",
    "    sentences = sent_tokenize(text)\n",
    "    \n",
    "    for sentence in sentences:\n",
    "        for crop in crops:\n",
    "            if crop.lower() in sentence.lower():\n",
    "                crops_advice[crop] += sentence + \" \"\n",
    "    \n",
    "    return crops_advice\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "ad440047-b5dc-436b-8ad3-3a3a95a73e84",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "expected string or bytes-like object",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[108], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Extract crop and advice\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m crops_advice \u001b[38;5;241m=\u001b[39m \u001b[43mextract_crop_advice\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcrop_data\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkeys\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Create a DataFrame\u001b[39;00m\n\u001b[0;32m      5\u001b[0m df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(\u001b[38;5;28mlist\u001b[39m(crops_advice\u001b[38;5;241m.\u001b[39mitems()), columns\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCrop\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAdvice\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "Cell \u001b[1;32mIn[107], line 4\u001b[0m, in \u001b[0;36mextract_crop_advice\u001b[1;34m(text, crops)\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mextract_crop_advice\u001b[39m(text, crops):\n\u001b[0;32m      3\u001b[0m     crops_advice \u001b[38;5;241m=\u001b[39m {crop: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m crop \u001b[38;5;129;01min\u001b[39;00m crops}\n\u001b[1;32m----> 4\u001b[0m     sentences \u001b[38;5;241m=\u001b[39m \u001b[43msent_tokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      6\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m sentence \u001b[38;5;129;01min\u001b[39;00m sentences:\n\u001b[0;32m      7\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m crop \u001b[38;5;129;01min\u001b[39;00m crops:\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\nltk\\tokenize\\__init__.py:107\u001b[0m, in \u001b[0;36msent_tokenize\u001b[1;34m(text, language)\u001b[0m\n\u001b[0;32m     97\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     98\u001b[0m \u001b[38;5;124;03mReturn a sentence-tokenized copy of *text*,\u001b[39;00m\n\u001b[0;32m     99\u001b[0m \u001b[38;5;124;03musing NLTK's recommended sentence tokenizer\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    104\u001b[0m \u001b[38;5;124;03m:param language: the model name in the Punkt corpus\u001b[39;00m\n\u001b[0;32m    105\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    106\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m load(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtokenizers/punkt/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlanguage\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.pickle\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 107\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\nltk\\tokenize\\punkt.py:1281\u001b[0m, in \u001b[0;36mPunktSentenceTokenizer.tokenize\u001b[1;34m(self, text, realign_boundaries)\u001b[0m\n\u001b[0;32m   1277\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtokenize\u001b[39m(\u001b[38;5;28mself\u001b[39m, text: \u001b[38;5;28mstr\u001b[39m, realign_boundaries: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[\u001b[38;5;28mstr\u001b[39m]:\n\u001b[0;32m   1278\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1279\u001b[0m \u001b[38;5;124;03m    Given a text, returns a list of the sentences in that text.\u001b[39;00m\n\u001b[0;32m   1280\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1281\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msentences_from_text\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrealign_boundaries\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\nltk\\tokenize\\punkt.py:1341\u001b[0m, in \u001b[0;36mPunktSentenceTokenizer.sentences_from_text\u001b[1;34m(self, text, realign_boundaries)\u001b[0m\n\u001b[0;32m   1332\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msentences_from_text\u001b[39m(\n\u001b[0;32m   1333\u001b[0m     \u001b[38;5;28mself\u001b[39m, text: \u001b[38;5;28mstr\u001b[39m, realign_boundaries: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m   1334\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[\u001b[38;5;28mstr\u001b[39m]:\n\u001b[0;32m   1335\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1336\u001b[0m \u001b[38;5;124;03m    Given a text, generates the sentences in that text by only\u001b[39;00m\n\u001b[0;32m   1337\u001b[0m \u001b[38;5;124;03m    testing candidate sentence breaks. If realign_boundaries is\u001b[39;00m\n\u001b[0;32m   1338\u001b[0m \u001b[38;5;124;03m    True, includes in the sentence closing punctuation that\u001b[39;00m\n\u001b[0;32m   1339\u001b[0m \u001b[38;5;124;03m    follows the period.\u001b[39;00m\n\u001b[0;32m   1340\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1341\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [text[s:e] \u001b[38;5;28;01mfor\u001b[39;00m s, e \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mspan_tokenize(text, realign_boundaries)]\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\nltk\\tokenize\\punkt.py:1341\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m   1332\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msentences_from_text\u001b[39m(\n\u001b[0;32m   1333\u001b[0m     \u001b[38;5;28mself\u001b[39m, text: \u001b[38;5;28mstr\u001b[39m, realign_boundaries: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m   1334\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[\u001b[38;5;28mstr\u001b[39m]:\n\u001b[0;32m   1335\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1336\u001b[0m \u001b[38;5;124;03m    Given a text, generates the sentences in that text by only\u001b[39;00m\n\u001b[0;32m   1337\u001b[0m \u001b[38;5;124;03m    testing candidate sentence breaks. If realign_boundaries is\u001b[39;00m\n\u001b[0;32m   1338\u001b[0m \u001b[38;5;124;03m    True, includes in the sentence closing punctuation that\u001b[39;00m\n\u001b[0;32m   1339\u001b[0m \u001b[38;5;124;03m    follows the period.\u001b[39;00m\n\u001b[0;32m   1340\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1341\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [text[s:e] \u001b[38;5;28;01mfor\u001b[39;00m s, e \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mspan_tokenize(text, realign_boundaries)]\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\nltk\\tokenize\\punkt.py:1329\u001b[0m, in \u001b[0;36mPunktSentenceTokenizer.span_tokenize\u001b[1;34m(self, text, realign_boundaries)\u001b[0m\n\u001b[0;32m   1327\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m realign_boundaries:\n\u001b[0;32m   1328\u001b[0m     slices \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_realign_boundaries(text, slices)\n\u001b[1;32m-> 1329\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m sentence \u001b[38;5;129;01min\u001b[39;00m slices:\n\u001b[0;32m   1330\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m (sentence\u001b[38;5;241m.\u001b[39mstart, sentence\u001b[38;5;241m.\u001b[39mstop)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\nltk\\tokenize\\punkt.py:1459\u001b[0m, in \u001b[0;36mPunktSentenceTokenizer._realign_boundaries\u001b[1;34m(self, text, slices)\u001b[0m\n\u001b[0;32m   1446\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1447\u001b[0m \u001b[38;5;124;03mAttempts to realign punctuation that falls after the period but\u001b[39;00m\n\u001b[0;32m   1448\u001b[0m \u001b[38;5;124;03mshould otherwise be included in the same sentence.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1456\u001b[0m \u001b[38;5;124;03m    [\"(Sent1.)\", \"Sent2.\"].\u001b[39;00m\n\u001b[0;32m   1457\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1458\u001b[0m realign \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m-> 1459\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m sentence1, sentence2 \u001b[38;5;129;01min\u001b[39;00m _pair_iter(slices):\n\u001b[0;32m   1460\u001b[0m     sentence1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mslice\u001b[39m(sentence1\u001b[38;5;241m.\u001b[39mstart \u001b[38;5;241m+\u001b[39m realign, sentence1\u001b[38;5;241m.\u001b[39mstop)\n\u001b[0;32m   1461\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m sentence2:\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\nltk\\tokenize\\punkt.py:321\u001b[0m, in \u001b[0;36m_pair_iter\u001b[1;34m(iterator)\u001b[0m\n\u001b[0;32m    319\u001b[0m iterator \u001b[38;5;241m=\u001b[39m \u001b[38;5;28miter\u001b[39m(iterator)\n\u001b[0;32m    320\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 321\u001b[0m     prev \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    322\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[0;32m    323\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\nltk\\tokenize\\punkt.py:1431\u001b[0m, in \u001b[0;36mPunktSentenceTokenizer._slices_from_text\u001b[1;34m(self, text)\u001b[0m\n\u001b[0;32m   1429\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_slices_from_text\u001b[39m(\u001b[38;5;28mself\u001b[39m, text: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Iterator[\u001b[38;5;28mslice\u001b[39m]:\n\u001b[0;32m   1430\u001b[0m     last_break \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m-> 1431\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m match, context \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_match_potential_end_contexts(text):\n\u001b[0;32m   1432\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtext_contains_sentbreak(context):\n\u001b[0;32m   1433\u001b[0m             \u001b[38;5;28;01myield\u001b[39;00m \u001b[38;5;28mslice\u001b[39m(last_break, match\u001b[38;5;241m.\u001b[39mend())\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\nltk\\tokenize\\punkt.py:1395\u001b[0m, in \u001b[0;36mPunktSentenceTokenizer._match_potential_end_contexts\u001b[1;34m(self, text)\u001b[0m\n\u001b[0;32m   1393\u001b[0m previous_slice \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mslice\u001b[39m(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m   1394\u001b[0m previous_match \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1395\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m match \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_lang_vars\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mperiod_context_re\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfinditer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[0;32m   1396\u001b[0m \n\u001b[0;32m   1397\u001b[0m     \u001b[38;5;66;03m# Get the slice of the previous word\u001b[39;00m\n\u001b[0;32m   1398\u001b[0m     before_text \u001b[38;5;241m=\u001b[39m text[previous_slice\u001b[38;5;241m.\u001b[39mstop : match\u001b[38;5;241m.\u001b[39mstart()]\n\u001b[0;32m   1399\u001b[0m     index_after_last_space \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_last_whitespace_index(before_text)\n",
      "\u001b[1;31mTypeError\u001b[0m: expected string or bytes-like object"
     ]
    }
   ],
   "source": [
    "# Extract crop and advice\n",
    "crops_advice = extract_crop_advice(result, crop_data.keys())\n",
    "\n",
    "# Create a DataFrame\n",
    "df = pd.DataFrame(list(crops_advice.items()), columns=['Crop', 'Advice'])\n",
    "\n",
    "# Clean up the advice column\n",
    "df['Advice'] = df['Advice'].str.strip()\n",
    "\n",
    "# Remove rows with empty advice\n",
    "df = df[df['Advice'] != \"\"]\n",
    "\n",
    "# Sort the DataFrame by crop name\n",
    "df = df.sort_values('Crop')\n",
    "\n",
    "# Reset the index\n",
    "df = df.reset_index(drop=True)\n",
    "\n",
    "# Display the table\n",
    "display (df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "1d19d77b-7fdc-4896-8e8a-bda4cbb8f2d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optionally, save to CSV\n",
    "df.to_csv('crop_advice.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f691e1b7-c9f0-4364-8162-55678d09b16c",
   "metadata": {},
   "source": [
    "## 4.0 USER INTERFACE\n",
    "**OVERVIEW**: Creates Gradio user enterface to enable user to select a count, select crops to consider and input 4-digit planting year using keyboard."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66e87df3-9931-4893-861b-57a1390892a4",
   "metadata": {},
   "source": [
    "**IMPORTS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "76262a59-d773-4b94-9665-873852be5916",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Uses Gradio to build interface\n",
    "import gradio as gr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db4ef3f4-cdb2-4ae1-8466-412ce7ad9852",
   "metadata": {},
   "source": [
    "### 4.1 Input Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "30cc9935-c0cd-490a-af1d-c995fff8c8e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defines the inputs for counties, crops and seasons\n",
    "counties = [\"Alamance\", \"Alexander\", \"Alleghany\", \"Anson\", \"Ashe\", \"Avery\", \"Beaufort\", \"Bertie\", \"Bladen\", \"Brunswick\",\n",
    "            \"Buncombe\", \"Burke\", \"Cabarrus\", \"Caldwell\", \"Camden\", \"Carteret\", \"Caswell\", \"Catawba\", \"Chatham\",\n",
    "            \"Cherokee\", \"Chowan\", \"Clay\", \"Cleveland\", \"Columbus\", \"Craven\", \"Cumberland\", \"Currituck\", \"Dare\",\n",
    "            \"Davidson\", \"Davie\", \"Duplin\", \"Durham\", \"Edgecombe\", \"Forsyth\", \"Franklin\", \"Gaston\", \"Gates\", \"Graham\",\n",
    "            \"Granville\", \"Greene\", \"Guilford\", \"Halifax\", \"Harnett\", \"Haywood\", \"Henderson\", \"Hertford\", \"Hoke\", \"Hyde\",\n",
    "            \"Iredell\", \"Jackson\", \"Johnston\", \"Jones\", \"Lee\", \"Lenoir\", \"Lincoln\", \"Macon\", \"Madison\", \"Martin\",\n",
    "            \"McDowell\", \"Mecklenburg\", \"Mitchell\", \"Montgomery\", \"Moore\", \"Nash\", \"New Hanover\", \"Northampton\",\n",
    "            \"Onslow\", \"Orange\", \"Pamlico\", \"Pasquotank\", \"Pender\", \"Perquimans\", \"Person\", \"Pitt\", \"Polk\", \"Randolph\",\n",
    "            \"Richmond\", \"Robeson\", \"Rockingham\", \"Rowan\", \"Rutherford\", \"Sampson\", \"Scotland\", \"Stanly\", \"Stokes\",\n",
    "            \"Surry\", \"Swain\", \"Transylvania\", \"Tyrrell\", \"Union\", \"Vance\", \"Wake\", \"Warren\", \"Washington\", \"Watauga\",\n",
    "            \"Wayne\", \"Wilkes\", \"Wilson\", \"Yadkin\", \"Yancey\"]\n",
    "\n",
    "crops = ['Barley', 'Corn', 'Hay', 'Oats', 'Peanuts', 'Bell Peppers', 'Soybeans', 'Squash',\n",
    "         'Sweet Potatoes', 'Tobacco', 'Wheat']\n",
    "\n",
    "seasons = ['Spring', 'Summer', 'Fall']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf7b6da1-1e1d-41a2-a0d1-1068632116c3",
   "metadata": {},
   "source": [
    "### 4.2 Input Transformation\n",
    "\n",
    "NOTE: for demonstration purposes, we simulate forecasted seasional avg temperatures (degrees F), seasional avg precipitation (inches), and weeks of D2, D3 and D4 drought conditions _(i.e., Avg Fall Temp , Avg Spring Temp, Avg Summer Temp, Avg Winter Temp,Avg Fall Precip, Avg Spring Precip, Avg Summer Precip, Avg Winter Precip, Weeks of Severe drought (D2), Weeks of Extreme Drought (D3), Weeks of Exceptional Drought)._\n",
    "\n",
    "Actual forecast can be pulled in via API from Weather forecasting service."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8c101a4-fd40-4c57-bf43-33ddfb3e1a56",
   "metadata": {},
   "source": [
    "**Weather Forecast Simulation Function**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "b74bea4e-5691-46c2-b193-fd1187da6c99",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# function simuate weather forecast data\n",
    "def get_varied_factors():\n",
    "   \n",
    "    base_factors = [58.77, 59.8, 75.13, 42.6, 9.58, 11.03, 14.9, 8.83, 29, 15, 1]\n",
    "    \n",
    "    # Precipitation variation (first 4 values)\n",
    "    precipitation = np.array(base_factors[:4])\n",
    "    precipitation_variation = np.random.normal(0, 5, 4)  # Mean 0, std dev 5\n",
    "    varied_precipitation = np.maximum(precipitation + precipitation_variation, 0)  # Ensure non-negative\n",
    "    \n",
    "    # Temperature variation (next 4 values)\n",
    "    temperature = np.array(base_factors[4:8])\n",
    "    temperature_variation = np.random.normal(0, 2, 4)  # Mean 0, std dev 2\n",
    "    varied_temperature = temperature + temperature_variation\n",
    "    \n",
    "    # Weeks of drought variation (last 3 values)\n",
    "    drought_weeks = np.array(base_factors[8:])\n",
    "    drought_variation = np.random.randint(-2, 3, 3)  # Random integer between -2 and 2\n",
    "    varied_drought = np.maximum(drought_weeks + drought_variation, 0)  # Ensure non-negative\n",
    "    \n",
    "    # Combines all varied factors\n",
    "    varied_factors = np.concatenate([varied_precipitation, varied_temperature, varied_drought])\n",
    "\n",
    "    # returns factors array\n",
    "    return varied_factors.tolist()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "412dcf1b-4bf7-4594-b192-1b503caa7753",
   "metadata": {},
   "source": [
    "**User Interface Function**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "76d3977f-46d1-4fcb-aeb2-1eed5183ca92",
   "metadata": {},
   "outputs": [],
   "source": [
    "def user_interface(image, county, crop_list, selected_seasons, year):\n",
    "    # Retrieves simulated weather forecast data (in production this would be forcast data from an API call to Wx fcst svc)\n",
    "    factors = get_varied_factors()\n",
    "\n",
    "    # Converts user input into crop_prediction function input\n",
    "    crop_input = {\n",
    "        'Barley': 'BARLEY_$_ACRE',\n",
    "        'Corn': 'CORN_$_ACRE',\n",
    "        'Hay': 'HAY_$_ACRE',\n",
    "        'Oats': 'OATS_$_ACRE',\n",
    "        'Peanuts': 'PEANUTS_$_ACRE',\n",
    "        'Bell Peppers': 'PEPPERS,BELL_$_ACRE',\n",
    "        'Soybeans': 'SOYBEANS_$_ACRE',\n",
    "        'Squash': 'SQUASH_$_ACRE',\n",
    "        'Sweet Potatoes': 'SWEET_$_ACRE',\n",
    "        'Tobacco': 'TOBACCO_$_ACRE',\n",
    "        'Wheat': 'WHEAT_$_ACRE'\n",
    "    }\n",
    "   \n",
    "    # Creates dictionary to hold the prediction values for each crop\n",
    "    prediction_dict = {}\n",
    "   \n",
    "    for crop in crop_list:\n",
    "        # Function to convert crop name into CROPNAME_$_ACRE\n",
    "        model_input = crop_input[crop]\n",
    "\n",
    "        # Runs prediction model for given crop\n",
    "        cropname, prediction, avg20, conf = crop_prediction(model_input, factors)\n",
    "\n",
    "        # Appends results to prediction dictionary\n",
    "        prediction_dict[crop] = {\n",
    "            'prediction': prediction,\n",
    "            'average_20_year': avg20,\n",
    "            'confidence': conf\n",
    "        }\n",
    "    \n",
    " # Generate planting decisions based on predictions\n",
    "    decisions = process_crop_predictions(prediction_dict)\n",
    "\n",
    "    # Generate recommendations\n",
    "    recommendations = generate_recommendations(decisions)\n",
    "    \n",
    "    # Combine predictions, decisions, and recommendations\n",
    "    result = {\n",
    "        'predictions': prediction_dict,\n",
    "        'decisions': decisions,\n",
    "        'recommendations': recommendations\n",
    "    }\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acecb76c-c898-4d23-82d3-36412104cb03",
   "metadata": {},
   "outputs": [],
   "source": [
    "**Input-to-Output Function**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "9a76eb3b-d9ad-4b03-819d-6351be72ba6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_recommendations(raw_result):\n",
    "    query = \"\"\"Review the provided crop data dictionary and for each crop and respective action, create a brief narrative to describe the considerations shown. Supplement the considerations with any financial risk mitigation strategies or crop resilience advice for the respective crop.\"\"\"\n",
    "    \n",
    "    # Retrieve relevant context using the retriever\n",
    "    relevant_docs = retriever.get_relevant_documents(query)\n",
    "    context = \"\\n\".join([doc.page_content for doc in relevant_docs])\n",
    "    \n",
    "    # Run the chain\n",
    "    result = llm_chain.run(context=context, query=query, crop_data=str(raw_result))\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "458b118f-a52e-42b4-888d-3c1fe7a349aa",
   "metadata": {},
   "source": [
    "### 4.3 Interface Design"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "9038a76b-c126-4083-bcb9-7be326f5c7ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7870\n",
      "Running on public URL: https://b6c39352f06c11c8e4.gradio.live\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://b6c39352f06c11c8e4.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define output to Gradio interface\n",
    "outputs = gr.JSON(label=\"Crop Predictions and Recommendations\")\n",
    "\n",
    "# Launch the Gradio interface\n",
    "gr.Interface(\n",
    "    fn=user_interface,\n",
    "    inputs=[\n",
    "        gr.Image(value=\"Images/ui_image.png\", label=\"Farm Image\"),  \n",
    "        gr.Dropdown(choices=counties, label=\"Select County\"),\n",
    "        gr.CheckboxGroup(choices=crops, label=\"Crops to Consider\"),\n",
    "        gr.CheckboxGroup(choices=seasons, label=\"Planting Season(s)\", value=seasons),\n",
    "        gr.Number(label=\"Planting Year (YYYY)\", value=2025, minimum=2025, maximum=2035)\n",
    "    ],\n",
    "    outputs=outputs,\n",
    "    title=\"Crop Planning and Protection Plan Generator\"\n",
    ").launch(share=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa522d0b-5972-4560-9450-c5e532fa02f3",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# To Be Deleted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "f60bd55a-de46-4094-8508-dd0bf461dbc2",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7861\n",
      "Running on public URL: https://8d0168d9318003b5d3.gradio.live\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://8d0168d9318003b5d3.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Jamie\\anaconda3\\envs\\dev\\lib\\site-packages\\gradio\\queueing.py\", line 536, in process_events\n",
      "    response = await route_utils.call_process_api(\n",
      "  File \"C:\\Users\\Jamie\\anaconda3\\envs\\dev\\lib\\site-packages\\gradio\\route_utils.py\", line 288, in call_process_api\n",
      "    output = await app.get_blocks().process_api(\n",
      "  File \"C:\\Users\\Jamie\\anaconda3\\envs\\dev\\lib\\site-packages\\gradio\\blocks.py\", line 1931, in process_api\n",
      "    result = await self.call_function(\n",
      "  File \"C:\\Users\\Jamie\\anaconda3\\envs\\dev\\lib\\site-packages\\gradio\\blocks.py\", line 1516, in call_function\n",
      "    prediction = await anyio.to_thread.run_sync(  # type: ignore\n",
      "  File \"C:\\Users\\Jamie\\anaconda3\\envs\\dev\\lib\\site-packages\\anyio\\to_thread.py\", line 56, in run_sync\n",
      "    return await get_async_backend().run_sync_in_worker_thread(\n",
      "  File \"C:\\Users\\Jamie\\anaconda3\\envs\\dev\\lib\\site-packages\\anyio\\_backends\\_asyncio.py\", line 2134, in run_sync_in_worker_thread\n",
      "    return await future\n",
      "  File \"C:\\Users\\Jamie\\anaconda3\\envs\\dev\\lib\\site-packages\\anyio\\_backends\\_asyncio.py\", line 851, in run\n",
      "    result = context.run(func, *args)\n",
      "  File \"C:\\Users\\Jamie\\anaconda3\\envs\\dev\\lib\\site-packages\\gradio\\utils.py\", line 826, in wrapper\n",
      "    response = f(*args, **kwargs)\n",
      "TypeError: user_interface() takes 4 positional arguments but 5 were given\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# # Define the Gradio interface\n",
    "# inputs = [\n",
    "#     gr.Image(value=\"Images/ui_image.png\", label=\"Farm Image\"),  \n",
    "#     gr.Dropdown(choices=counties, label=\"Select County\"),\n",
    "#     gr.CheckboxGroup(choices=crops, label=\"Crops to Consider\"),\n",
    "#     gr.CheckboxGroup(choices=seasons, label=\"Planting Season(s)\", value=seasons),\n",
    "#     gr.Number(label=\"Planting Year (YYYY)\", value=2025, minimum=2025, maximum=2035)\n",
    "# ]\n",
    "\n",
    "# # Defines Output Design\n",
    "# outputs = gr.Textbox(label=\"Planting and Protection Recommendations\")\n",
    "\n",
    "# # Launches the Gradio interface\n",
    "# gr.Interface(fn=user_interface, inputs=inputs, outputs=outputs, title=\"Crop Planning and Protection Plan Generator\").launch(share=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dev",
   "language": "python",
   "name": "dev"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
