{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d35b42ec-e70f-4248-b73f-4297321abcb6",
   "metadata": {},
   "source": [
    "# AG CROP PLANNING TOOL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b60320f7-2ca5-420c-9795-37526c39fb23",
   "metadata": {},
   "source": [
    "The Ag Decision Engine accepts inputs from a user (Crop Type, Location and Timeframe) and develops a customized crop planning and protection plan for farmland owners or operatators across North Carolina. The decision engine offers a basic interface for user input and leverages ouputs from a crop performance prediction model and a RAG-enhanced LLM for recomendation building."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26e5bfc7-0449-444f-b4da-002a9058cf08",
   "metadata": {},
   "source": [
    "## CODE CONTENT\n",
    "\n",
    "**1.0 - USER INTERFACE (U/I)** \\\n",
    "1.1 - Input Definition\\\n",
    "1.2 - U/I Design\\\n",
    "\\\n",
    "**2.0 - CROP PREDICTION MODEL** \\\n",
    "2.1 - TBD\\\n",
    "2.2 - TBD\\\n",
    "\\\n",
    "**3.0 - DECISON LOGIC MODEL** \\\n",
    "3.1 - TBD\\\n",
    "3.2 - TBD\\\n",
    "\\\n",
    "**4.0 - RECOMENDATION BUILDER** \\\n",
    "4.1 - TBD\\\n",
    "4.2 - TBD"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f691e1b7-c9f0-4364-8162-55678d09b16c",
   "metadata": {},
   "source": [
    "## 1.0 USER INTERFACE\n",
    "**OVERVIEW**: Gradio interface enables user to select a count, select crops to consider and input 4-digit planting year using keyboard.\n",
    "\n",
    "**DEPENDENCIES**\n",
    "* _N/A_\n",
    "\n",
    "**INSTRUCTIONS**\n",
    "* _N/A_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66e87df3-9931-4893-861b-57a1390892a4",
   "metadata": {},
   "source": [
    "**IMPORTS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76262a59-d773-4b94-9665-873852be5916",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uses Gradio to build interface\n",
    "import gradio as gr\n",
    "\n",
    "# Removes unnecessary warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db4ef3f4-cdb2-4ae1-8466-412ce7ad9852",
   "metadata": {},
   "source": [
    "## 1.1 Input Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30cc9935-c0cd-490a-af1d-c995fff8c8e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defines the inputs for counties, crops and seasons\n",
    "counties = [\"Alamance\", \"Alexander\", \"Alleghany\", \"Anson\", \"Ashe\", \"Avery\", \"Beaufort\", \"Bertie\", \"Bladen\", \"Brunswick\",\n",
    "            \"Buncombe\", \"Burke\", \"Cabarrus\", \"Caldwell\", \"Camden\", \"Carteret\", \"Caswell\", \"Catawba\", \"Chatham\",\n",
    "            \"Cherokee\", \"Chowan\", \"Clay\", \"Cleveland\", \"Columbus\", \"Craven\", \"Cumberland\", \"Currituck\", \"Dare\",\n",
    "            \"Davidson\", \"Davie\", \"Duplin\", \"Durham\", \"Edgecombe\", \"Forsyth\", \"Franklin\", \"Gaston\", \"Gates\", \"Graham\",\n",
    "            \"Granville\", \"Greene\", \"Guilford\", \"Halifax\", \"Harnett\", \"Haywood\", \"Henderson\", \"Hertford\", \"Hoke\", \"Hyde\",\n",
    "            \"Iredell\", \"Jackson\", \"Johnston\", \"Jones\", \"Lee\", \"Lenoir\", \"Lincoln\", \"Macon\", \"Madison\", \"Martin\",\n",
    "            \"McDowell\", \"Mecklenburg\", \"Mitchell\", \"Montgomery\", \"Moore\", \"Nash\", \"New Hanover\", \"Northampton\",\n",
    "            \"Onslow\", \"Orange\", \"Pamlico\", \"Pasquotank\", \"Pender\", \"Perquimans\", \"Person\", \"Pitt\", \"Polk\", \"Randolph\",\n",
    "            \"Richmond\", \"Robeson\", \"Rockingham\", \"Rowan\", \"Rutherford\", \"Sampson\", \"Scotland\", \"Stanly\", \"Stokes\",\n",
    "            \"Surry\", \"Swain\", \"Transylvania\", \"Tyrrell\", \"Union\", \"Vance\", \"Wake\", \"Warren\", \"Washington\", \"Watauga\",\n",
    "            \"Wayne\", \"Wilkes\", \"Wilson\", \"Yadkin\", \"Yancey\"]\n",
    "\n",
    "crops = ['Barley', 'Corn', 'Cotton', 'Hay', 'Oats', 'Peanuts', 'Bell Peppers', 'Pumpkins', 'Soybeans', 'Squash',\n",
    "         'Sweet Potatoes', 'Tobacco', 'Wheat']\n",
    "\n",
    "seasons = ['Spring', 'Summer', 'Fall']\n",
    "\n",
    "# Function for formating inputs\n",
    "def crop_prediction(county, crop_list, selected_seasons, year):\n",
    "    # Placeholder function to simulate crop prediction\n",
    "    crop_yields = [1.0] * len(crop_list)\n",
    "    crop_values = [2.0] * len(crop_list)\n",
    "    confidence_levels = [0.8] * len(crop_list)\n",
    "    return crop_yields, crop_values, confidence_levels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "458b118f-a52e-42b4-888d-3c1fe7a349aa",
   "metadata": {},
   "source": [
    "## 1.2 U/I Design"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f60bd55a-de46-4094-8508-dd0bf461dbc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function defining Gradio Interface\n",
    "def user_interface(county, crop_list, selected_seasons, year):\n",
    "    # Call the crop prediction model with user inputs\n",
    "    crop_yields, crop_values, confidence_levels = crop_prediction(county, crop_list, selected_seasons, year)\n",
    "    \n",
    "    # Display results (for now, just showing the inputs for demonstration)\n",
    "    return f\"County: {county}\\nCrops: {', '.join(crop_list)}\\nSeasons: {', '.join(selected_seasons)}\\nYear: {year}\"\n",
    "\n",
    "# Define the Gradio interface\n",
    "inputs = [\n",
    "    gr.Image(value=\"Images/ui_image.png\", label=\"Farm Image\"),  \n",
    "    gr.Dropdown(choices=counties, label=\"Select County\"),\n",
    "    gr.CheckboxGroup(choices=crops, label=\"Crops to Consider\"),\n",
    "    gr.CheckboxGroup(choices=seasons, label=\"Planting Season(s)\", value=seasons),\n",
    "    gr.Number(label=\"Planting Year (YYYY)\", value=2025, minimum=2025, maximum=2035)\n",
    "]\n",
    "\n",
    "# Defines Output Design\n",
    "outputs = gr.Textbox(label=\"Planting and Protection Recommendations\")\n",
    "\n",
    "# Launches the Gradio interface\n",
    "gr.Interface(fn=user_interface, inputs=inputs, outputs=outputs, title=\"Crop Planning and Protection Plan Generator\").launch(share=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "696e934a-a487-4ba0-b614-40f9d4782a26",
   "metadata": {},
   "source": [
    "## 2.0 CROP PREDICTION MODEL\n",
    "**OVERVIEW**: Trains ML model on historical agriculture data (i.e., 12 NC crops grown in North Carolina between 2000 and 2020). Training features comprised of annual and seasonal temperature and precipitation data. Training targets were production value ($) per acre and yield per acre.\n",
    "\n",
    "**DEPENDENCIES**\\\n",
    "[dependencies]\n",
    "\n",
    "**INSTRUCTIONS**\\\n",
    "[instructions]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00128c6c-92dc-4188-aa65-a00818df20e5",
   "metadata": {},
   "source": [
    "**IMPORTS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f126d60f-0f77-4973-92a1-a6ac4a36092f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insert Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53dbfd8c-fca6-46e6-97c7-3ad09e10a092",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c59b6760-901f-486f-bb16-49695b7be8f8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a0655cc9-98a4-4c6a-9301-3a65a8429e87",
   "metadata": {},
   "source": [
    "## 4.0 RECOMENDATION BUILDER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0ef78ef-c8d3-41d4-8f16-722dcee63cf1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "407cfbe6-1141-4848-aafa-9a0ff208901c",
   "metadata": {},
   "source": [
    "## 4.0 RECOMENDATION BUILDER\n",
    "**OVERVIEW**: Accepts dataframe variable contining crop performance and associated justifications. Generates recommendation narrative for each crop using LLM. Supplements recommendation with additional considerations and mitagation information retrieve from RAG.\r\n",
    "\n",
    "**DEPENDENCIES**\n",
    "* Natural Lanugage Processing\n",
    "    * Local LLM:  Ollama _([dowload]('https://ollama.com/download/windows'))_ running 'phi3:mini' model _([documentation]('https://ollama.com/library/phi3'))_\n",
    "    * Hosted LLM: OpenAI _([documentation]('https://platform.openai.com/docs/overview'))_ _(ALTERNATIVE)_ \n",
    "* Document Loading, Embedding and Retrieval\n",
    "    * LangChain _([documentation]('https://python.langchain.com/v0.2/docs/introduction/')) loads and splits documents_\n",
    "    * Unstructured _([documentation]('https://docs.unstructured.io/welcome')) pre-processes pdf documents_\n",
    "    * OpenAI _([documentation]('https://platform.openai.com/docs/guides/embeddings/')) converts documents into embeddings_\n",
    "    * ChromaDB _([documentation]('https://docs.trychroma.com/getting-started')) stores embeddings_\n",
    "\n",
    "\n",
    "**INSTRUCTIONS**\n",
    "1.  Start the Ollama service by running the following command: `ollama serve`\n",
    "2.  Allow Ollama service to run in the background while running code\n",
    "3.  Pull the latest update to the Ollama phi3 model by running the following command:`ollama pull phi3:mini`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7b82eb2-591f-459c-9dee-cab9441fc358",
   "metadata": {},
   "source": [
    "**IMPORTS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8ed8ffea-11f5-4f37-842f-a9931205e188",
   "metadata": {},
   "outputs": [],
   "source": [
    "import path # supports file paths\n",
    "import os # supports use of environment variables\n",
    "import time\n",
    "from tqdm import tqdm # supports progress monitoring\n",
    "\n",
    "# Assumes use of local LLM (if using hosted LLM use libaries below instead)\n",
    "import ollama\n",
    "from langchain.llms import Ollama\n",
    "from langchain.embeddings import OllamaEmbeddings\n",
    "\n",
    "# # Uncomment if using hosted LLM (OpenAI)\n",
    "# import openai # for hosted LLM option\n",
    "# from langchain import OpenAI\n",
    "# from langchain.embeddings import OpenAIEmbeddings\n",
    "\n",
    "# for loading various document types\n",
    "from langchain_community.document_loaders import PyPDFLoader, BSHTMLLoader, UnstructuredFileLoader, DirectoryLoader\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.document_loaders import DirectoryLoader\n",
    "\n",
    "# Libraries for prompting and parsing\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.output_parsers import RegexParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9a107a10-c49d-40f1-a02f-a2a3bbd77aa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment code below if using hosted LLM (OpenAI)\n",
    "\n",
    "# # Helper function for loading API key\n",
    "# from dotenv import load_dotenv, find_dotenv\n",
    "# _ = load_dotenv(find_dotenv()) # reads local .env file\n",
    "\n",
    "# # Loads variable environment for API Key\n",
    "# openai.api_key = os.environ['OPENAI_API_KEY']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22cee742-b354-4aae-91a1-47dc93475e13",
   "metadata": {},
   "source": [
    "## Document Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "683d22ae-76dc-403f-9bf4-b8723b3ae270",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current working directory: C:\\Users\\Jamie\\OneDrive\\desktop\\AI_Bootcamp\\MOD_23_Project_3\\AgProject3\n"
     ]
    }
   ],
   "source": [
    "# Checks current directory path (helps user ensure correct documents_path set\n",
    "current_dir = os.getcwd()\n",
    "print(\"Current working directory:\", current_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaba47dc-1159-45be-8730-0b4d5c84d5d1",
   "metadata": {},
   "source": [
    "**USER NOTE**: Variable below (documents_path) must be modified to reflect location of RAG documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0c3ca2a9-7098-4c6d-a2c6-7b06e18b76b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loads documents from current working directory\n",
    "documents_path = './rag_content' # EDIT PATH FOR NEW DIRECTORY AS NEEDED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b63293c3-e503-4e9b-b30e-7f41d4f1eb76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to support html files docs with different encodings\n",
    "class CustomHTMLLoader(UnstructuredFileLoader):\n",
    "    def __init__(self, file_path: str):\n",
    "        super().__init__(file_path)\n",
    "\n",
    "    def _get_elements(self):\n",
    "        try:\n",
    "            with open(self.file_path, 'r', encoding='utf-8') as f:\n",
    "                content = f.read()\n",
    "        except UnicodeDecodeError:\n",
    "            try:\n",
    "                with open(self.file_path, 'r', encoding='latin-1') as f:\n",
    "                    content = f.read()\n",
    "            except UnicodeDecodeError:\n",
    "                with open(self.file_path, 'r', encoding='cp1252') as f:\n",
    "                    content = f.read()\n",
    "        \n",
    "        soup = BeautifulSoup(content, 'html.parser')\n",
    "        text = soup.get_text(separator='\\n', strip=True)\n",
    "        return [text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "276b4601-5c25-494b-bc99-275faba87034",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to support loading text files with different encodings\n",
    "class CustomTextLoader(UnstructuredFileLoader):\n",
    "    def __init__(self, file_path: str):\n",
    "        super().__init__(file_path)\n",
    "\n",
    "    def _get_elements(self):\n",
    "        try:\n",
    "            with open(self.file_path, 'r', encoding='utf-8') as f:\n",
    "                text = f.read()\n",
    "        except UnicodeDecodeError:\n",
    "            try:\n",
    "                with open(self.file_path, 'r', encoding='latin-1') as f:\n",
    "                    text = f.read()\n",
    "            except UnicodeDecodeError:\n",
    "                with open(self.file_path, 'r', encoding='cp1252') as f:\n",
    "                    text = f.read()\n",
    "        return [text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e1f9520f-97de-4dd1-8d62-9056260cfb81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory found: ./rag_content\n",
      "Loading documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files: 100%|██████████████████████████████████████████████████████████████████| 6/6 [00:00<00:00,  8.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 10 documents\n",
      "Encountered 0 errors\n",
      "\n",
      "Summary of loaded documents:\n",
      "pdf: 7\n",
      "html: 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Sets up loaders for different file types\n",
    "loaders = {\n",
    "    \"**/*.pdf\": PyPDFLoader,\n",
    "    \"**/*.html\": CustomHTMLLoader,\n",
    "    \"**/*.txt\": CustomTextLoader\n",
    "}\n",
    "# Check if the directory exists\n",
    "if not os.path.exists(documents_path):\n",
    "    print(f\"Directory not found: {documents_path}\")\n",
    "    print(\"Contents of current directory:\")\n",
    "    print(os.listdir(os.getcwd()))\n",
    "    raise FileNotFoundError(f\"Directory {documents_path} does not exist\")\n",
    "\n",
    "print(f\"Directory found: {documents_path}\")\n",
    "\n",
    "# Function to get the appropriate loader\n",
    "def get_loader(file_path):\n",
    "    for glob_pattern, loader_class in loaders.items():\n",
    "        if file_path.endswith(glob_pattern.split(\"*\")[-1]):\n",
    "            return loader_class(file_path)\n",
    "    return CustomTextLoader(file_path)  # Default to CustomTextLoader\n",
    "\n",
    "# Load documents\n",
    "print(\"Loading documents...\")\n",
    "documents = []\n",
    "errors = []\n",
    "\n",
    "for root, _, files in os.walk(documents_path):\n",
    "    for file in tqdm(files, desc=\"Processing files\"):\n",
    "        file_path = os.path.join(root, file)\n",
    "        try:\n",
    "            loader = get_loader(file_path)\n",
    "            docs = loader.load()\n",
    "            documents.extend(docs)\n",
    "        except Exception as e:\n",
    "            errors.append((file_path, str(e)))\n",
    "\n",
    "print(f\"Loaded {len(documents)} documents\")\n",
    "print(f\"Encountered {len(errors)} errors\")\n",
    "\n",
    "if errors:\n",
    "    print(\"\\nErrors encountered:\")\n",
    "    for file_path, error in errors:\n",
    "        print(f\"{file_path}: {error}\")\n",
    "\n",
    "# Print summary of loaded documents\n",
    "file_types = {}\n",
    "for doc in documents:\n",
    "    file_type = os.path.splitext(doc.metadata.get('source', ''))[-1].lstrip('.')\n",
    "    file_types[file_type] = file_types.get(file_type, 0) + 1\n",
    "\n",
    "print(\"\\nSummary of loaded documents:\")\n",
    "for file_type, count in file_types.items():\n",
    "    print(f\"{file_type}: {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b32d969b-c529-46ef-b62f-9b3f41214b93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Loads pdf from website\n",
    "# link = \"https://www.rma.usda.gov/sites/default/files/topics/good_farming_practices.pdf\"\n",
    "# !wget link -O good_farming_practices.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "067d5b3a-30ee-4414-af36-27d8da5199e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the documents into smaller chunks for better processing\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "docs = text_splitter.split_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b3995d00-dc23-4ef3-b734-3955fd3d4cb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Splitting documents: 100%|███████████████████████████████████████████████████████████| 10/10 [00:00<00:00, 2845.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing document 1/10\n",
      "Document 1 size: 5276 characters\n",
      "Document 1/10 processed:\n",
      "  - Chunks created: 7\n",
      "  - Time taken: 0.00 seconds\n",
      "  - Processing speed: N/A (processed too quickly to measure)\n",
      "Total time elapsed: 0.00 seconds\n",
      "\n",
      "Processing document 2/10\n",
      "Document 2 size: 6632 characters\n",
      "Document 2/10 processed:\n",
      "  - Chunks created: 9\n",
      "  - Time taken: 0.00 seconds\n",
      "  - Processing speed: 6574479.82 characters/second\n",
      "Total time elapsed: 0.00 seconds\n",
      "\n",
      "Processing document 3/10\n",
      "Document 3 size: 1079 characters\n",
      "Document 3/10 processed:\n",
      "  - Chunks created: 2\n",
      "  - Time taken: 0.00 seconds\n",
      "  - Processing speed: N/A (processed too quickly to measure)\n",
      "Total time elapsed: 0.00 seconds\n",
      "\n",
      "Processing document 4/10\n",
      "Document 4 size: 18854 characters\n",
      "Document 4/10 processed:\n",
      "  - Chunks created: 24\n",
      "  - Time taken: 0.00 seconds\n",
      "  - Processing speed: 18882380.04 characters/second\n",
      "Total time elapsed: 0.00 seconds\n",
      "\n",
      "Processing document 5/10\n",
      "Document 5 size: 2475 characters\n",
      "Document 5/10 processed:\n",
      "  - Chunks created: 3\n",
      "  - Time taken: 0.00 seconds\n",
      "  - Processing speed: N/A (processed too quickly to measure)\n",
      "Total time elapsed: 0.00 seconds\n",
      "\n",
      "Processing document 6/10\n",
      "Document 6 size: 2447 characters\n",
      "Document 6/10 processed:\n",
      "  - Chunks created: 3\n",
      "  - Time taken: 0.00 seconds\n",
      "  - Processing speed: N/A (processed too quickly to measure)\n",
      "Total time elapsed: 0.00 seconds\n",
      "\n",
      "Processing document 7/10\n",
      "Document 7 size: 2671 characters\n",
      "Document 7/10 processed:\n",
      "  - Chunks created: 4\n",
      "  - Time taken: 0.00 seconds\n",
      "  - Processing speed: N/A (processed too quickly to measure)\n",
      "Total time elapsed: 0.00 seconds\n",
      "\n",
      "Processing document 8/10\n",
      "Document 8 size: 2265 characters\n",
      "Document 8/10 processed:\n",
      "  - Chunks created: 3\n",
      "  - Time taken: 0.00 seconds\n",
      "  - Processing speed: 2260851.63 characters/second\n",
      "Total time elapsed: 0.01 seconds\n",
      "\n",
      "Processing document 9/10\n",
      "Document 9 size: 5901 characters\n",
      "Document 9/10 processed:\n",
      "  - Chunks created: 7\n",
      "  - Time taken: 0.00 seconds\n",
      "  - Processing speed: N/A (processed too quickly to measure)\n",
      "Total time elapsed: 0.01 seconds\n",
      "\n",
      "Processing document 10/10\n",
      "Document 10 size: 3923 characters\n",
      "Document 10/10 processed:\n",
      "  - Chunks created: 5\n",
      "  - Time taken: 0.00 seconds\n",
      "  - Processing speed: N/A (processed too quickly to measure)\n",
      "Total time elapsed: 0.01 seconds\n",
      "\n",
      "Text splitting complete!\n",
      "Total documents processed: 10\n",
      "Total chunks created: 67\n",
      "Total characters processed: 51523\n",
      "Total time taken: 0.01 seconds\n",
      "Overall processing speed: 7331494.27 characters/second\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Splits the documents into smaller chunks for better processing\n",
    "# Initializes the text splitter\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "\n",
    "# Initializes an empty list to store the split documents\n",
    "start_time = time.time()\n",
    "split_docs = []\n",
    "total_chunks = 0\n",
    "\n",
    "# Monitors text splitter progress\n",
    "for i, doc in enumerate(tqdm(documents, desc=\"Splitting documents\")):\n",
    "    # Prints count number of document being processed size of the document before processing\n",
    "    print(f\"\\nProcessing document {i+1}/{len(documents)}\")\n",
    "    print(f\"Document {i+1} size: {len(doc.page_content)} characters\")\n",
    "    \n",
    "    # Performs the text splitting\n",
    "    doc_start_time = time.time()\n",
    "    split_doc = text_splitter.split_documents([doc])\n",
    "    split_docs.extend(split_doc)\n",
    "    \n",
    "    # Calculates and prints statistics for the current document\n",
    "    doc_time = time.time() - doc_start_time\n",
    "    chunks_created = len(split_doc)\n",
    "    total_chunks += chunks_created\n",
    "    \n",
    "    print(f\"Document {i+1}/{len(documents)} processed:\")\n",
    "    print(f\"  - Chunks created: {chunks_created}\")\n",
    "    print(f\"  - Time taken: {doc_time:.2f} seconds\")\n",
    "    \n",
    "    # Avoid division by zero\n",
    "    if doc_time > 0:\n",
    "        print(f\"  - Processing speed: {len(doc.page_content) / doc_time:.2f} characters/second\")\n",
    "    else:\n",
    "        print(f\"  - Processing speed: N/A (processed too quickly to measure)\")\n",
    "    \n",
    "    print(f\"Total time elapsed: {time.time() - start_time:.2f} seconds\")\n",
    "\n",
    "# Final statistics\n",
    "total_time = time.time() - start_time\n",
    "total_characters = sum(len(doc.page_content) for doc in documents)\n",
    "\n",
    "print(\"\\nText splitting complete!\")\n",
    "print(f\"Total documents processed: {len(documents)}\")\n",
    "print(f\"Total chunks created: {total_chunks}\")\n",
    "print(f\"Total characters processed: {total_characters}\")\n",
    "print(f\"Total time taken: {total_time:.2f} seconds\")\n",
    "\n",
    "# Avoid division by zero in overall statistics\n",
    "if total_time > 0:\n",
    "    print(f\"Overall processing speed: {total_characters / total_time:.2f} characters/second\")\n",
    "else:\n",
    "    print(\"Overall processing speed: N/A (processed too quickly to measure)\")\n",
    "\n",
    "# Now split_docs contains all the split documents\n",
    "docs = split_docs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "139874d4-4b5f-4057-a9c2-74382a13f8e7",
   "metadata": {},
   "source": [
    "## Text Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "95ff1a59-216a-4faa-8b65-a90809084a23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate embeddings for the document chunks\n",
    "print(\"Generating embeddings and creating vector store...\")\n",
    "start_time = time.time()\n",
    "\n",
    "\n",
    "# Use Ollama for embeddings (NOTE: Use the second line below instead if using hosted LLM) \n",
    "embeddings = OllamaEmbeddings(model=\"phi3:mini\")\n",
    "# embeddings = OpenAIEmbeddings() \n",
    "\n",
    "\n",
    "vector_store = Chroma.from_documents(docs, embeddings)\n",
    "\n",
    "# Create a progress bar\n",
    "pbar = tqdm(total=len(docs), desc=\"Processing documents\")\n",
    "\n",
    "def embed_function(texts):\n",
    "    results = embeddings.embed_documents(texts)\n",
    "    pbar.update(len(texts))\n",
    "    return results\n",
    "\n",
    "# Create the vector store with the custom embed_function\n",
    "vector_store = Chroma.from_documents(docs, embeddings, embed_documents=embed_function)\n",
    "\n",
    "pbar.close()\n",
    "\n",
    "end_time = time.time()\n",
    "total_time = end_time - start_time\n",
    "\n",
    "print(f\"\\nEmbedding generation and vector store creation completed.\")\n",
    "print(f\"Total time taken: {total_time:.2f} seconds\")\n",
    "print(f\"Average time per document: {total_time/len(docs):.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "741839c1-c30c-4617-92c5-141c342279bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a retriever using the vector store\n",
    "retriever = vector_store.as_retriever()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23e691db-c9e6-4497-b7ff-073cba2b4861",
   "metadata": {},
   "source": [
    "## Prompt Design"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "00f1711c-24db-428c-9889-b67e1c2d9156",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a prompt template\n",
    "prompt_template = PromptTemplate(\n",
    "    input_variables=[\"query\"],\n",
    "    template=\"You are an agricultural specialist who advises farmers on how to optimize farm operations and mitigate against weather and climate disasters. Please respond to the following: {query}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "557d2181-31a0-41e6-b6c1-5c711dd48c9d",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValidationError",
     "evalue": "1 validation error for StuffDocumentsChain\n__root__\n  document_variable_name context was not found in llm_chain input_variables: ['query'] (type=value_error)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValidationError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[31], line 5\u001b[0m\n\u001b[0;32m      2\u001b[0m llm \u001b[38;5;241m=\u001b[39m Ollama(model\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mphi3:mini\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Set up the RetrievalQA chain\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m qa_chain \u001b[38;5;241m=\u001b[39m \u001b[43mRetrievalQA\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_chain_type\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mllm\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mllm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchain_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstuff\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretriever\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretriever\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_source_documents\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchain_type_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\n\u001b[0;32m     11\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mprompt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt_template\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43m}\u001b[49m\n\u001b[0;32m     13\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\dev\\lib\\site-packages\\langchain\\chains\\retrieval_qa\\base.py:114\u001b[0m, in \u001b[0;36mBaseRetrievalQA.from_chain_type\u001b[1;34m(cls, llm, chain_type, chain_type_kwargs, **kwargs)\u001b[0m\n\u001b[0;32m    112\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Load chain from chain type.\"\"\"\u001b[39;00m\n\u001b[0;32m    113\u001b[0m _chain_type_kwargs \u001b[38;5;241m=\u001b[39m chain_type_kwargs \u001b[38;5;129;01mor\u001b[39;00m {}\n\u001b[1;32m--> 114\u001b[0m combine_documents_chain \u001b[38;5;241m=\u001b[39m load_qa_chain(\n\u001b[0;32m    115\u001b[0m     llm, chain_type\u001b[38;5;241m=\u001b[39mchain_type, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m_chain_type_kwargs\n\u001b[0;32m    116\u001b[0m )\n\u001b[0;32m    117\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m(combine_documents_chain\u001b[38;5;241m=\u001b[39mcombine_documents_chain, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\dev\\lib\\site-packages\\langchain_core\\_api\\deprecation.py:170\u001b[0m, in \u001b[0;36mdeprecated.<locals>.deprecate.<locals>.warning_emitting_wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    168\u001b[0m     warned \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    169\u001b[0m     emit_warning()\n\u001b[1;32m--> 170\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m wrapped(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\dev\\lib\\site-packages\\langchain\\chains\\question_answering\\chain.py:265\u001b[0m, in \u001b[0;36mload_qa_chain\u001b[1;34m(llm, chain_type, verbose, callback_manager, **kwargs)\u001b[0m\n\u001b[0;32m    260\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chain_type \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m loader_mapping:\n\u001b[0;32m    261\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    262\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGot unsupported chain type: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mchain_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    263\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mShould be one of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloader_mapping\u001b[38;5;241m.\u001b[39mkeys()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    264\u001b[0m     )\n\u001b[1;32m--> 265\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loader_mapping[chain_type](\n\u001b[0;32m    266\u001b[0m     llm, verbose\u001b[38;5;241m=\u001b[39mverbose, callback_manager\u001b[38;5;241m=\u001b[39mcallback_manager, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    267\u001b[0m )\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\dev\\lib\\site-packages\\langchain\\chains\\question_answering\\chain.py:83\u001b[0m, in \u001b[0;36m_load_stuff_chain\u001b[1;34m(llm, prompt, document_variable_name, verbose, callback_manager, callbacks, **kwargs)\u001b[0m\n\u001b[0;32m     75\u001b[0m llm_chain \u001b[38;5;241m=\u001b[39m LLMChain(\n\u001b[0;32m     76\u001b[0m     llm\u001b[38;5;241m=\u001b[39mllm,\n\u001b[0;32m     77\u001b[0m     prompt\u001b[38;5;241m=\u001b[39m_prompt,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     80\u001b[0m     callbacks\u001b[38;5;241m=\u001b[39mcallbacks,\n\u001b[0;32m     81\u001b[0m )\n\u001b[0;32m     82\u001b[0m \u001b[38;5;66;03m# TODO: document prompt\u001b[39;00m\n\u001b[1;32m---> 83\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m StuffDocumentsChain(\n\u001b[0;32m     84\u001b[0m     llm_chain\u001b[38;5;241m=\u001b[39mllm_chain,\n\u001b[0;32m     85\u001b[0m     document_variable_name\u001b[38;5;241m=\u001b[39mdocument_variable_name,\n\u001b[0;32m     86\u001b[0m     verbose\u001b[38;5;241m=\u001b[39mverbose,  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[0;32m     87\u001b[0m     callback_manager\u001b[38;5;241m=\u001b[39mcallback_manager,\n\u001b[0;32m     88\u001b[0m     callbacks\u001b[38;5;241m=\u001b[39mcallbacks,\n\u001b[0;32m     89\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m     90\u001b[0m )\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\dev\\lib\\site-packages\\langchain_core\\_api\\deprecation.py:205\u001b[0m, in \u001b[0;36mdeprecated.<locals>.deprecate.<locals>.finalize.<locals>.warn_if_direct_instance\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    203\u001b[0m     warned \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    204\u001b[0m     emit_warning()\n\u001b[1;32m--> 205\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m wrapped(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\dev\\lib\\site-packages\\langchain_core\\load\\serializable.py:113\u001b[0m, in \u001b[0;36mSerializable.__init__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    111\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    112\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\"\"\"\u001b[39;00m\n\u001b[1;32m--> 113\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\dev\\lib\\site-packages\\pydantic\\v1\\main.py:341\u001b[0m, in \u001b[0;36mBaseModel.__init__\u001b[1;34m(__pydantic_self__, **data)\u001b[0m\n\u001b[0;32m    339\u001b[0m values, fields_set, validation_error \u001b[38;5;241m=\u001b[39m validate_model(__pydantic_self__\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m, data)\n\u001b[0;32m    340\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m validation_error:\n\u001b[1;32m--> 341\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m validation_error\n\u001b[0;32m    342\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    343\u001b[0m     object_setattr(__pydantic_self__, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__dict__\u001b[39m\u001b[38;5;124m'\u001b[39m, values)\n",
      "\u001b[1;31mValidationError\u001b[0m: 1 validation error for StuffDocumentsChain\n__root__\n  document_variable_name context was not found in llm_chain input_variables: ['query'] (type=value_error)"
     ]
    }
   ],
   "source": [
    "# if using Ollama \n",
    "llm = Ollama(model=\"phi3:mini\")\n",
    "\n",
    "# Set up the RetrievalQA chain\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=retriever,\n",
    "    return_source_documents=True,\n",
    "    chain_type_kwargs={\n",
    "        \"prompt\": prompt_template,\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2cbcf78-194e-4311-8051-d8ac69b0a135",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the RetrievalQA chain\n",
    "qa_chain = RetrievalQA(\n",
    "    llm=OpenAI(),  # Use Ollama if available, otherwise OpenAI\n",
    "    retriever=retriever\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04254abf-bba5-46ce-80a5-57dd363ca8e4",
   "metadata": {},
   "source": [
    "### Output Parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f663c1f6-4bf8-48f8-b89e-11e5caf63866",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define an output parser\n",
    "output_parser = RegexParser(\n",
    "    pattern=r\"Answer: (.*)\",\n",
    "    output_keys=[\"answer\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c966ed9-a8ab-46d7-adeb-4f7a0a22503d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # If using OpenAI\n",
    "# # Set up the RetrievalQA chain with prompt template and output parser\n",
    "# qa_chain = RetrievalQA(\n",
    "#     llm=OpenAI(),  # Use Ollama if available, otherwise OpenAI\n",
    "#     retriever=retriever,\n",
    "#     prompt_template=prompt_template,\n",
    "#     output_parser=output_parser\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6db92f62-9509-4ca0-94fc-e698bf0de310",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Example query\n",
    "query = \"What is the capital of France?\"\n",
    "formatted_query = prompt_template.format(query=query)\n",
    "response = qa_chain.run(formatted_query)\n",
    "parsed_response = output_parser.parse(response)\n",
    "\n",
    "print(f\"Response: {parsed_response['answer']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5d208c3-6d5e-4976-8b1d-eb68e712b77b",
   "metadata": {},
   "source": [
    "### Direct RAG Query "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad0670de-c227-4aee-98b1-d59c094d80c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example query\n",
    "query = \"What is the capital of France?\"\n",
    "response = qa_chain.run(query)\n",
    "print(f\"Response: {response}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "682c4ed3-ba9f-4802-bb0c-baa1fb7c46c3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dev",
   "language": "python",
   "name": "dev"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
