{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d208fc50-5ae0-4e95-b242-26ff1489ab26",
   "metadata": {},
   "source": [
    "# AG DECISION ENGINE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ffdd821-03a7-44ba-9625-56fbc94d2d8e",
   "metadata": {},
   "source": [
    "The Ag Decision Engine accepts inputs from a user (Crop Type, Location and Timeframe) and develops a customized crop planning and protection plan for farmland owners or operatators across North Carolina. The decision engine offers a basic interface for <em><u>user input</u></em> and leverages ouputs from a <em><u>crop performance prediction model</u></em> and a RAG-enhanced LLM for recomendation building."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61477eae-c507-451b-a751-4a7368780f73",
   "metadata": {},
   "source": [
    "## 1.0 User Interface"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8766463f-522f-4800-b663-e37ec6ca8b88",
   "metadata": {},
   "source": [
    "**OVERVIEW**\\\r\n",
    "To the tool, a user must input a county, crops to consider and target planting season.\r\n",
    "\\\r\n",
    "The user interface, built using gradio, is designed to accept three sets of user inputs: \r\n",
    "* INPUT 1: “Select County\" -  user inputs a NC county from a dropdown list\r\n",
    "* INPUT 2: “Crops To Consider\" -  user inputs crop types by selecting appropriate checkboxes\r\n",
    "* INPUT 3: “Planting Season(s) and Year (‘YYYY’)” – user inputs select relevant seasons using a set of checkbox and input a 4-digit year (between 2025 and 2035) using a keypad\r\n",
    "\r\n",
    "\\\r\n",
    "**CODE CONTENT**\\\r\n",
    "1.0 - U/I Dependencies\\\r\n",
    "1.1 - Input Definition\\\r\n",
    "1.2 - U/I Design"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e78ba5d8-7812-48ba-8fe5-5e3e1d131a9c",
   "metadata": {},
   "source": [
    "**U/I DEPENDENCIES**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9dc5e131-0e70-4056-a23f-1c0c45a3217f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uses Gradio to build interface\n",
    "import path\n",
    "import gradio as gr\n",
    "\n",
    "# Removes unnecessary warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56af12a1-ada5-4c1e-83f4-c432021f0f70",
   "metadata": {},
   "source": [
    "## 1.1 Input Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8564d2d0-5523-447c-85ce-4ee1e0868a84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defines the inputs for counties, crops and seasons\n",
    "counties = [\"Alamance\", \"Alexander\", \"Alleghany\", \"Anson\", \"Ashe\", \"Avery\", \"Beaufort\", \"Bertie\", \"Bladen\", \"Brunswick\",\n",
    "            \"Buncombe\", \"Burke\", \"Cabarrus\", \"Caldwell\", \"Camden\", \"Carteret\", \"Caswell\", \"Catawba\", \"Chatham\",\n",
    "            \"Cherokee\", \"Chowan\", \"Clay\", \"Cleveland\", \"Columbus\", \"Craven\", \"Cumberland\", \"Currituck\", \"Dare\",\n",
    "            \"Davidson\", \"Davie\", \"Duplin\", \"Durham\", \"Edgecombe\", \"Forsyth\", \"Franklin\", \"Gaston\", \"Gates\", \"Graham\",\n",
    "            \"Granville\", \"Greene\", \"Guilford\", \"Halifax\", \"Harnett\", \"Haywood\", \"Henderson\", \"Hertford\", \"Hoke\", \"Hyde\",\n",
    "            \"Iredell\", \"Jackson\", \"Johnston\", \"Jones\", \"Lee\", \"Lenoir\", \"Lincoln\", \"Macon\", \"Madison\", \"Martin\",\n",
    "            \"McDowell\", \"Mecklenburg\", \"Mitchell\", \"Montgomery\", \"Moore\", \"Nash\", \"New Hanover\", \"Northampton\",\n",
    "            \"Onslow\", \"Orange\", \"Pamlico\", \"Pasquotank\", \"Pender\", \"Perquimans\", \"Person\", \"Pitt\", \"Polk\", \"Randolph\",\n",
    "            \"Richmond\", \"Robeson\", \"Rockingham\", \"Rowan\", \"Rutherford\", \"Sampson\", \"Scotland\", \"Stanly\", \"Stokes\",\n",
    "            \"Surry\", \"Swain\", \"Transylvania\", \"Tyrrell\", \"Union\", \"Vance\", \"Wake\", \"Warren\", \"Washington\", \"Watauga\",\n",
    "            \"Wayne\", \"Wilkes\", \"Wilson\", \"Yadkin\", \"Yancey\"]\n",
    "\n",
    "crops = ['Barley', 'Corn', 'Cotton', 'Hay', 'Oats', 'Peanuts', 'Bell Peppers', 'Pumpkins', 'Soybeans', 'Squash',\n",
    "         'Sweet Potatoes', 'Tobacco', 'Wheat']\n",
    "\n",
    "seasons = ['Spring', 'Summer', 'Fall']\n",
    "\n",
    "# Function for formating inputs\n",
    "def crop_prediction(county, crop_list, selected_seasons, year):\n",
    "    # Placeholder function to simulate crop prediction\n",
    "    crop_yields = [1.0] * len(crop_list)\n",
    "    crop_values = [2.0] * len(crop_list)\n",
    "    confidence_levels = [0.8] * len(crop_list)\n",
    "    return crop_yields, crop_values, confidence_levels\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80c2b925-cf82-486b-a01f-c227896e54d9",
   "metadata": {},
   "source": [
    "## 1.2 U/I Design"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a85f06d4-69df-4063-988a-fd3e872ad7a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7860\n",
      "Running on public URL: https://994eeedba894e3b2e3.gradio.live\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://994eeedba894e3b2e3.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Function defining Gradio Interface\n",
    "def user_interface(county, crop_list, selected_seasons, year):\n",
    "    # Call the crop prediction model with user inputs\n",
    "    crop_yields, crop_values, confidence_levels = crop_prediction(county, crop_list, selected_seasons, year)\n",
    "    \n",
    "    # Display results (for now, just showing the inputs for demonstration)\n",
    "    return f\"County: {county}\\nCrops: {', '.join(crop_list)}\\nSeasons: {', '.join(selected_seasons)}\\nYear: {year}\"\n",
    "\n",
    "# Define the Gradio interface\n",
    "inputs = [\n",
    "    gr.Image(value=\"Images/ui_image.png\", label=\"Farm Image\"),  \n",
    "    gr.Dropdown(choices=counties, label=\"Select County\"),\n",
    "    gr.CheckboxGroup(choices=crops, label=\"Crops to Consider\"),\n",
    "    gr.CheckboxGroup(choices=seasons, label=\"Planting Season(s)\", value=seasons),\n",
    "    gr.Number(label=\"Planting Year (YYYY)\", value=2025, minimum=2025, maximum=2035)\n",
    "]\n",
    "\n",
    "# Defines Output Design\n",
    "outputs = gr.Textbox(label=\"Planting and Protection Recommendations\")\n",
    "\n",
    "# Launches the Gradio interface\n",
    "gr.Interface(fn=user_interface, inputs=inputs, outputs=outputs, title=\"Crop Planning and Protection Plan Generator\").launch(share=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca0b016d-0877-4071-9aa9-18a0bf968857",
   "metadata": {},
   "source": [
    "## 2.0 Retrival Augmented Generation (RAG) Support"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c58dcd8-3ea9-46d7-ab85-c2bac351f981",
   "metadata": {},
   "source": [
    "**OVERVIEW**\\\n",
    "To improve the Ag Decision Engine recommendation building, we use a retrival augmented generation (RAG) design to enhance our existing LLM with USDA, NC Department of Agriculture and other related content.\\\n",
    "\\\n",
    "The multimodal RAG is designed to perform the following tasks:  \n",
    "* Parsing text, tables and images from pdf documents using [Unstructured](https://unstructured.io/) \n",
    "* Generating text summaries from parsed content using [OpenAI GPT-4V](https://openai.com/index/gpt-4v-system-card/) \n",
    "* Embedding summarized content using [LancChain multi-vector retriever](https://python.langchain.com/docs/modules/data_connection/retrievers/multi_vector) \n",
    "* Storing and retrieving embedded summaries (with a reference to any raw images) using [Chroma](https://www.trychroma.com/) \n",
    "* Generating LLM query response from text chunks and raw images using [OpenAI GPT-4V](https://openai.com/index/gpt-4v-system-card/) \n",
    "\n",
    "\\\n",
    "**CODE CONTENT**\\\n",
    "2.0 - RAG Dependencie\\\n",
    "2.1 - RAG ChatModel\\\n",
    "2.2 - RAG Document Loader\\\n",
    "2.3 - RAG Text Summarizer\\\n",
    "2.4 - RAG VectorStore and Retriever\n",
    "\n",
    "\\\n",
    "**ATTRIBUTIONS**\\\n",
    "RAG code adapted from [LangChain Cookbook: Multi-modal RAG](https://github.com/langchain-ai/langchain/blob/master/cookbook/Multi_modal_RAG.ipynb).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffcebeda-2f78-448c-a140-909dfefb7c17",
   "metadata": {},
   "source": [
    "**RAG DEPENDENCIES (OPEN SOURCE PACKAGES)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4116446d-a634-4aac-92a0-46c91de00fad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install pdf2image #(includes poppler)\n",
    "# ! pip install -U langchain openai langchain-chroma langchain-experimental # (requires newest version for multi-modal)\n",
    "# ! pip install \"unstructured[all-docs]\" pillow pydantic lxml pillow matplotlib chromadb tiktoken\n",
    "# pip install -U langchain-openai"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbfe1183-9b62-4c5c-9a44-ff0656cd7f02",
   "metadata": {},
   "source": [
    "Additional packages may be required _(see installation instructions linked)_\n",
    "* [poppler](https://pdf2image.readthedocs.io/en/latest/installation.html) required for pdf handling\n",
    "* [tesseract](https://tesseract-ocr.github.io/tessdoc/Installation.html) required for OCR text recognition\n",
    "* [pytorch](https://pytorch.org/get-started/locally/#windows-installation) required when ingesting data via the [Unstructured](https://docs.unstructured.io/welcome) package"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6136471b-ccdd-48d2-bfd3-160dabfd9e9d",
   "metadata": {},
   "source": [
    "**RAG DEPENDENCIES (LIBRARIES)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3434403e-3498-4014-a61c-9a8fc7c67e85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uses OpenAI LLM\n",
    "import os # Needed to access API key\n",
    "import openai\n",
    "\n",
    "# LLM Chat\n",
    "from langchain.chat_models import ChatOpenAI # Needed for chat model functionality\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate # Needed for using LangChain prompt templates\n",
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "# Data Loading\n",
    "from langchain_text_splitters import CharacterTextSplitter # Needed for data loading, partitioning PDFs, tables, text, and images\n",
    "from unstructured.partition.pdf import partition_pdf # Needed for partitioning PDFs, tables, text, and images\n",
    "\n",
    "# Text, Table and Image Summaries\n",
    "from langchain_core.output_parsers import StrOutputParser # Needed for summary generation\n",
    "import base64\n",
    "\n",
    "# Vectorstores\n",
    "import uuid # Needed for generating document ids\n",
    "from langchain.retrievers.multi_vector import MultiVectorRetriever # Needed to perform vector retrieval\n",
    "from langchain.storage import InMemoryStore # Need to support proper content storage\n",
    "from langchain_chroma import Chroma # Needed for accessing vector database\n",
    "from langchain_core.documents import Document # Needed to \n",
    "from langchain_openai import OpenAIEmbeddings # Needed to generate vector embeddings\n",
    "\n",
    "# RAG Retrieval\n",
    "import io\n",
    "import re\n",
    "from IPython.display import HTML, display\n",
    "from langchain_core.runnables import RunnableLambda, RunnablePassthrough\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f6a60b65-5013-455e-a4c9-fe6ad89e9631",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function for loading API key\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "_ = load_dotenv(find_dotenv()) # reads local .env file\n",
    "\n",
    "openai.api_key = os.environ['OPENAI_API_KEY']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2b63509-2e34-4a98-b017-eedd04d19b36",
   "metadata": {},
   "source": [
    "## 2.1 RAG ChatModel Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "337652a5-c76d-411f-a557-5d83c19b2352",
   "metadata": {},
   "source": [
    "**LLM Setup (Cloud)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "081e5211-27bd-4c75-a74d-d3dd0ef1dd78",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatOpenAI(client=<openai.resources.chat.completions.Completions object at 0x000001A93ECAD600>, async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x000001A93ECAEEF0>, root_client=<openai.OpenAI object at 0x000001A93E6A3610>, root_async_client=<openai.AsyncOpenAI object at 0x000001A93ECAD630>, model_name='gpt-4o-mini', temperature=0.0, openai_api_key=SecretStr('**********'), openai_proxy='')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# OpenAI model selection\n",
    "llm_model = \"gpt-4o-mini\"\n",
    "\n",
    "# Uses temperature = 0.0 to reduce randomness in retrieved responses\n",
    "chat = ChatOpenAI(temperature=0.0, model=llm_model)\n",
    "\n",
    "# Optional - shows custom chat settings\n",
    "chat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6499636-da6a-42dd-9035-5be792612251",
   "metadata": {},
   "source": [
    "**LLM Setup (Local)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2756c910-6f30-4a12-9e6d-a5ddcbfb893a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f90a136d-4f32-4e8b-85c1-8f5ee747e215",
   "metadata": {},
   "source": [
    "## 2.2 RAG Document Loader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e31dadd-3a75-4b18-a69c-0ca7433a938a",
   "metadata": {},
   "source": [
    "**Partitioning Functions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1d43bad8-cf75-4b0d-b478-015acebc2c71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracts elements from PDF\n",
    "def extract_pdf_elements(path, fname):\n",
    "    \"\"\"\n",
    "    Extract images, tables, and chunk text from a PDF file.\n",
    "    path: File path, which is used to dump images (.jpg)\n",
    "    fname: File name\n",
    "    \"\"\"\n",
    "    return partition_pdf(\n",
    "        filename=path + fname,\n",
    "        extract_images_in_pdf=False,\n",
    "        infer_table_structure=True,\n",
    "        chunking_strategy=\"by_title\",\n",
    "        max_characters=4000,\n",
    "        new_after_n_chars=3800,\n",
    "        combine_text_under_n_chars=2000,\n",
    "        image_output_dir_path=path,\n",
    "    )\n",
    "\n",
    "\n",
    "# Categorizes elements by type\n",
    "def categorize_elements(raw_pdf_elements):\n",
    "    \"\"\"\n",
    "    Categorize extracted elements from a PDF into tables and texts.\n",
    "    raw_pdf_elements: List of unstructured.documents.elements\n",
    "    \"\"\"\n",
    "    tables = []\n",
    "    texts = []\n",
    "    for element in raw_pdf_elements:\n",
    "        if \"unstructured.documents.elements.Table\" in str(type(element)):\n",
    "            tables.append(str(element))\n",
    "        elif \"unstructured.documents.elements.CompositeElement\" in str(type(element)):\n",
    "            texts.append(str(element))\n",
    "    return texts, tables\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ecb7326-0732-436a-ac44-1131c15fa090",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accesses documents\n",
    "# File path\n",
    "fpath = \"/Users/rlm/Desktop/cj/\"\n",
    "fname = \"cj.pdf\"\n",
    "\n",
    "# Get elements\n",
    "raw_pdf_elements = extract_pdf_elements(fpath, fname)\n",
    "\n",
    "# Get text, tables\n",
    "texts, tables = categorize_elements(raw_pdf_elements)\n",
    "\n",
    "# Optional: Enforce a specific token size for texts\n",
    "text_splitter = CharacterTextSplitter.from_tiktoken_encoder(\n",
    "    chunk_size=4000, chunk_overlap=0\n",
    ")\n",
    "joined_texts = \" \".join(texts)\n",
    "texts_4k_token = text_splitter.split_text(joined_texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7456af2c-ce59-4e0d-834a-6a21cc3f4f3f",
   "metadata": {},
   "source": [
    "## 2.3 RAG Text Summarizers\n",
    "Uses multi-vector-retriever to index image (and / or text, table) summaries,\\\n",
    "but retrieve raw images (along with raw texts or tables).\\\n",
    "Sourced from LangChain Cookbook (_https://github.com/langchain-ai/langchain/blob/master/cookbook/Multi_modal_RAG.ipynb_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fadf3a3-7c30-4fc5-8410-f37d1899e007",
   "metadata": {},
   "source": [
    "**Text and Table Summary Function**\\\n",
    "Uses GPT-4 to produce table and text summaries used to retrieve raw tables and raw chunks of text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd40bc94-a32c-43ea-ba0e-016897e5e533",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate summaries of text elements\n",
    "def generate_text_summaries(texts, tables, summarize_texts=False):\n",
    "    \"\"\"\n",
    "    Summarize text elements\n",
    "    texts: List of str\n",
    "    tables: List of str\n",
    "    summarize_texts: Bool to summarize texts\n",
    "    \"\"\"\n",
    "\n",
    "    # Prompt\n",
    "    prompt_text = \"\"\"You are an assistant tasked with summarizing tables and text for retrieval. \\\n",
    "    These summaries will be embedded and used to retrieve the raw text or table elements. \\\n",
    "    Give a concise summary of the table or text that is well optimized for retrieval. Table or text: {element} \"\"\"\n",
    "    prompt = ChatPromptTemplate.from_template(prompt_text)\n",
    "\n",
    "    # Text summary chain\n",
    "    model = ChatOpenAI(temperature=0, model=\"gpt-4\")\n",
    "    summarize_chain = {\"element\": lambda x: x} | prompt | model | StrOutputParser()\n",
    "\n",
    "    # Initialize empty summaries\n",
    "    text_summaries = []\n",
    "    table_summaries = []\n",
    "\n",
    "    # Apply to text if texts are provided and summarization is requested\n",
    "    if texts and summarize_texts:\n",
    "        text_summaries = summarize_chain.batch(texts, {\"max_concurrency\": 5})\n",
    "    elif texts:\n",
    "        text_summaries = texts\n",
    "\n",
    "    # Apply to tables if tables are provided\n",
    "    if tables:\n",
    "        table_summaries = summarize_chain.batch(tables, {\"max_concurrency\": 5})\n",
    "\n",
    "    return text_summaries, table_summaries\n",
    "\n",
    "\n",
    "# Get text, table summaries\n",
    "text_summaries, table_summaries = generate_text_summaries(\n",
    "    texts_4k_token, tables, summarize_texts=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09ed3d45-b800-4055-b529-3c220ebe3ce6",
   "metadata": {},
   "source": [
    "**Image Summary Function**\\\n",
    "Uses GPT-4V to produce the image summaries. See API documenation here: https://platform.openai.com/docs/guides/vision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb61a52a-dff5-4cd2-bafc-a171d58c08f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import base64\n",
    "\n",
    "\n",
    "# def encode_image(image_path):\n",
    "#     \"\"\"Getting the base64 string\"\"\"\n",
    "#     with open(image_path, \"rb\") as image_file:\n",
    "#         return base64.b64encode(image_file.read()).decode(\"utf-8\")\n",
    "\n",
    "\n",
    "# def image_summarize(img_base64, prompt):\n",
    "#     \"\"\"Make image summary\"\"\"\n",
    "#     chat = ChatOpenAI(model=\"gpt-4-vision-preview\", max_tokens=1024)\n",
    "\n",
    "#     msg = chat.invoke(\n",
    "#         [\n",
    "#             HumanMessage(\n",
    "#                 content=[\n",
    "#                     {\"type\": \"text\", \"text\": prompt},\n",
    "#                     {\n",
    "#                         \"type\": \"image_url\",\n",
    "#                         \"image_url\": {\"url\": f\"data:image/jpeg;base64,{img_base64}\"},\n",
    "#                     },\n",
    "#                 ]\n",
    "#             )\n",
    "#         ]\n",
    "#     )\n",
    "#     return msg.content\n",
    "\n",
    "\n",
    "# def generate_img_summaries(path):\n",
    "#     \"\"\"\n",
    "#     Generate summaries and base64 encoded strings for images\n",
    "#     path: Path to list of .jpg files extracted by Unstructured\n",
    "#     \"\"\"\n",
    "\n",
    "#     # Store base64 encoded images\n",
    "#     img_base64_list = []\n",
    "\n",
    "#     # Store image summaries\n",
    "#     image_summaries = []\n",
    "\n",
    "#     # Prompt\n",
    "#     prompt = \"\"\"You are an assistant tasked with summarizing images for retrieval. \\\n",
    "#     These summaries will be embedded and used to retrieve the raw image. \\\n",
    "#     Give a concise summary of the image that is well optimized for retrieval.\"\"\"\n",
    "\n",
    "#     # Apply to images\n",
    "#     for img_file in sorted(os.listdir(path)):\n",
    "#         if img_file.endswith(\".jpg\"):\n",
    "#             img_path = os.path.join(path, img_file)\n",
    "#             base64_image = encode_image(img_path)\n",
    "#             img_base64_list.append(base64_image)\n",
    "#             image_summaries.append(image_summarize(base64_image, prompt))\n",
    "\n",
    "#     return img_base64_list, image_summaries\n",
    "\n",
    "\n",
    "# # Image summaries\n",
    "# img_base64_list, image_summaries = generate_img_summaries(fpath)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40292e62-c91c-4d6f-aab8-ed2bcfdfc3f1",
   "metadata": {},
   "source": [
    "## 2.4 RAG VectorStore and Retriever\n",
    "Sourced from LangChain Cookbook (_https://github.com/langchain-ai/langchain/blob/master/cookbook/Multi_modal_RAG.ipynb_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1d9c9f1-634f-452d-af7e-e5c675555935",
   "metadata": {},
   "source": [
    "**Content Storage Function**\\\n",
    "Stores raw texts, tables, and images in the docstore; Stores texts, table and image summaries in the vectorstore for efficient semantic retrieval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4e03acf-6bb5-434f-b439-82d78b5560f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_multi_vector_retriever(\n",
    "    vectorstore, text_summaries, texts, table_summaries, tables, image_summaries, images\n",
    "):\n",
    "    \"\"\"\n",
    "    Create retriever that indexes summaries, but returns raw images or texts\n",
    "    \"\"\"\n",
    "\n",
    "    # Initialize the storage layer\n",
    "    store = InMemoryStore()\n",
    "    id_key = \"doc_id\"\n",
    "\n",
    "    # Create the multi-vector retriever\n",
    "    retriever = MultiVectorRetriever(\n",
    "        vectorstore=vectorstore,\n",
    "        docstore=store,\n",
    "        id_key=id_key,\n",
    "    )\n",
    "\n",
    "    # Helper function to add documents to the vectorstore and docstore\n",
    "    def add_documents(retriever, doc_summaries, doc_contents):\n",
    "        doc_ids = [str(uuid.uuid4()) for _ in doc_contents]\n",
    "        summary_docs = [\n",
    "            Document(page_content=s, metadata={id_key: doc_ids[i]})\n",
    "            for i, s in enumerate(doc_summaries)\n",
    "        ]\n",
    "        retriever.vectorstore.add_documents(summary_docs)\n",
    "        retriever.docstore.mset(list(zip(doc_ids, doc_contents)))\n",
    "\n",
    "    # Add texts, tables, and images\n",
    "    # Check that text_summaries is not empty before adding\n",
    "    if text_summaries:\n",
    "        add_documents(retriever, text_summaries, texts)\n",
    "    # Check that table_summaries is not empty before adding\n",
    "    if table_summaries:\n",
    "        add_documents(retriever, table_summaries, tables)\n",
    "    # Check that image_summaries is not empty before adding\n",
    "    if image_summaries:\n",
    "        add_documents(retriever, image_summaries, images)\n",
    "\n",
    "    return retriever\n",
    "\n",
    "\n",
    "# The vectorstore to use to index the summaries\n",
    "vectorstore = Chroma(\n",
    "    collection_name=\"mm_rag_cj_blog\", embedding_function=OpenAIEmbeddings()\n",
    ")\n",
    "\n",
    "# Create retriever\n",
    "retriever_multi_vector_img = create_multi_vector_retriever(\n",
    "    vectorstore,\n",
    "    text_summaries,\n",
    "    texts,\n",
    "    table_summaries,\n",
    "    tables,\n",
    "    image_summaries,\n",
    "    img_base64_list,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a9d08b3-b3e9-4c95-862e-9d0ed1b8c62f",
   "metadata": {},
   "source": [
    "**Content Retrieval Functions**\\\n",
    "Bins the retrieved doc(s) into the correct parts of the GPT-4V prompt template."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "292872a0-c3d5-42c0-a50f-b9ff8268c4ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def plt_img_base64(img_base64):\n",
    "    \"\"\"Disply base64 encoded string as image\"\"\"\n",
    "    # Create an HTML img tag with the base64 string as the source\n",
    "    image_html = f'<img src=\"data:image/jpeg;base64,{img_base64}\" />'\n",
    "    # Display the image by rendering the HTML\n",
    "    display(HTML(image_html))\n",
    "\n",
    "\n",
    "def looks_like_base64(sb):\n",
    "    \"\"\"Check if the string looks like base64\"\"\"\n",
    "    return re.match(\"^[A-Za-z0-9+/]+[=]{0,2}$\", sb) is not None\n",
    "\n",
    "\n",
    "def is_image_data(b64data):\n",
    "    \"\"\"\n",
    "    Check if the base64 data is an image by looking at the start of the data\n",
    "    \"\"\"\n",
    "    image_signatures = {\n",
    "        b\"\\xff\\xd8\\xff\": \"jpg\",\n",
    "        b\"\\x89\\x50\\x4e\\x47\\x0d\\x0a\\x1a\\x0a\": \"png\",\n",
    "        b\"\\x47\\x49\\x46\\x38\": \"gif\",\n",
    "        b\"\\x52\\x49\\x46\\x46\": \"webp\",\n",
    "    }\n",
    "    try:\n",
    "        header = base64.b64decode(b64data)[:8]  # Decode and get the first 8 bytes\n",
    "        for sig, format in image_signatures.items():\n",
    "            if header.startswith(sig):\n",
    "                return True\n",
    "        return False\n",
    "    except Exception:\n",
    "        return False\n",
    "\n",
    "\n",
    "def resize_base64_image(base64_string, size=(128, 128)):\n",
    "    \"\"\"\n",
    "    Resize an image encoded as a Base64 string\n",
    "    \"\"\"\n",
    "    # Decode the Base64 string\n",
    "    img_data = base64.b64decode(base64_string)\n",
    "    img = Image.open(io.BytesIO(img_data))\n",
    "\n",
    "    # Resize the image\n",
    "    resized_img = img.resize(size, Image.LANCZOS)\n",
    "\n",
    "    # Save the resized image to a bytes buffer\n",
    "    buffered = io.BytesIO()\n",
    "    resized_img.save(buffered, format=img.format)\n",
    "\n",
    "    # Encode the resized image to Base64\n",
    "    return base64.b64encode(buffered.getvalue()).decode(\"utf-8\")\n",
    "\n",
    "\n",
    "def split_image_text_types(docs):\n",
    "    \"\"\"\n",
    "    Split base64-encoded images and texts\n",
    "    \"\"\"\n",
    "    b64_images = []\n",
    "    texts = []\n",
    "    for doc in docs:\n",
    "        # Check if the document is of type Document and extract page_content if so\n",
    "        if isinstance(doc, Document):\n",
    "            doc = doc.page_content\n",
    "        if looks_like_base64(doc) and is_image_data(doc):\n",
    "            doc = resize_base64_image(doc, size=(1300, 600))\n",
    "            b64_images.append(doc)\n",
    "        else:\n",
    "            texts.append(doc)\n",
    "    return {\"images\": b64_images, \"texts\": texts}\n",
    "\n",
    "\n",
    "def img_prompt_func(data_dict):\n",
    "    \"\"\"\n",
    "    Join the context into a single string\n",
    "    \"\"\"\n",
    "    formatted_texts = \"\\n\".join(data_dict[\"context\"][\"texts\"])\n",
    "    messages = []\n",
    "\n",
    "    # Adding image(s) to the messages if present\n",
    "    if data_dict[\"context\"][\"images\"]:\n",
    "        for image in data_dict[\"context\"][\"images\"]:\n",
    "            image_message = {\n",
    "                \"type\": \"image_url\",\n",
    "                \"image_url\": {\"url\": f\"data:image/jpeg;base64,{image}\"},\n",
    "            }\n",
    "            messages.append(image_message)\n",
    "\n",
    "    # Adding the text for analysis\n",
    "    text_message = {\n",
    "        \"type\": \"text\",\n",
    "        \"text\": (\n",
    "            \"You are financial analyst tasking with providing investment advice.\\n\"\n",
    "            \"You will be given a mixed of text, tables, and image(s) usually of charts or graphs.\\n\"\n",
    "            \"Use this information to provide investment advice related to the user question. \\n\"\n",
    "            f\"User-provided question: {data_dict['question']}\\n\\n\"\n",
    "            \"Text and / or tables:\\n\"\n",
    "            f\"{formatted_texts}\"\n",
    "        ),\n",
    "    }\n",
    "    messages.append(text_message)\n",
    "    return [HumanMessage(content=messages)]\n",
    "\n",
    "\n",
    "def multi_modal_rag_chain(retriever):\n",
    "    \"\"\"\n",
    "    Multi-modal RAG chain\n",
    "    \"\"\"\n",
    "\n",
    "    # Multi-modal LLM\n",
    "    model = ChatOpenAI(temperature=0, model=\"gpt-4-vision-preview\", max_tokens=1024)\n",
    "\n",
    "    # RAG pipeline\n",
    "    chain = (\n",
    "        {\n",
    "            \"context\": retriever | RunnableLambda(split_image_text_types),\n",
    "            \"question\": RunnablePassthrough(),\n",
    "        }\n",
    "        | RunnableLambda(img_prompt_func)\n",
    "        | model\n",
    "        | StrOutputParser()\n",
    "    )\n",
    "\n",
    "    return chain\n",
    "\n",
    "\n",
    "# Create RAG chain\n",
    "chain_multimodal_rag = multi_modal_rag_chain(retriever_multi_vector_img)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afc4fc1d-d1a6-4bb6-8f9e-7520f7e37c76",
   "metadata": {},
   "source": [
    "# X.X USING RAG CAPABILITY"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83bd08cd-5914-4af8-838f-875194a975f2",
   "metadata": {},
   "source": [
    "**Retrieval Example**\\\n",
    "Examines retrieval to check that images are relevant to query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "348ee2e7-0c82-443c-a708-44f7cc0d075a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check retrieval\n",
    "query = \"Give me company names that are interesting investments based on EV / NTM and NTM rev growth. Consider EV / NTM multiples vs historical?\"\n",
    "docs = retriever_multi_vector_img.invoke(query, limit=6)\n",
    "\n",
    "# We get 4 docs\n",
    "len(docs)\n",
    "\n",
    "# Check retrieval\n",
    "query = \"What are the EV / NTM and NTM rev growth for MongoDB, Cloudflare, and Datadog?\"\n",
    "docs = retriever_multi_vector_img.invoke(query, limit=6)\n",
    "\n",
    "# Checks number of documents returned\n",
    "len(docs)\n",
    "\n",
    "# # Returns relevant images and image summary\n",
    "# plt_img_base64(docs[0])\n",
    "# image_summaries[3]\n",
    "\n",
    "# Runs RAG and test ability to synthesize an answer to question.\n",
    "chain_multimodal_rag.invoke(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79f3e837-3f2c-4e40-bc2f-8cd1f18f882f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6cb26e5-78ed-4e16-b9fb-1faa3b0f203c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dev",
   "language": "python",
   "name": "dev"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
