{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d208fc50-5ae0-4e95-b242-26ff1489ab26",
   "metadata": {},
   "source": [
    "# CREATING MULTIMODAL RAG CAPABILITY"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c58dcd8-3ea9-46d7-ab85-c2bac351f981",
   "metadata": {},
   "source": [
    "**OVERVIEW**\\\n",
    "Retrival augmented generation (RAG) combines LLM capability with external content to improve LLM outputs. The this notebook supports multimodal RAG capabilities via the combination of functions built to perform the following tasks: \n",
    "* Parsing text, tables and images from pdf documents using [Unstructured](https://unstructured.io/) \n",
    "* Using a LLM to produce summaries of parsed content (text, tables, images) using [OpenAI GPT-4V](https://openai.com/index/gpt-4v-system-card/) \n",
    "* Embedding summarized content using [LancChain multi-vector retriever](https://python.langchain.com/docs/modules/data_connection/retrievers/multi_vector) \n",
    "* Storing and retrieving summaries (with a reference to any raw images) using [Chroma](https://www.trychroma.com/) \n",
    "* Generating LLM query response from text chunks and raw images using [OpenAI GPT-4V](https://openai.com/index/gpt-4v-system-card/) \n",
    "\n",
    "**ATTRIBUTIONS**\\\n",
    "They python code for these functions is sourced from [LangChain Cookbook: Multi-modal RAG](https://github.com/langchain-ai/langchain/blob/master/cookbook/Multi_modal_RAG.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71876d96-f58e-4392-8695-91d3fc0e5fa7",
   "metadata": {},
   "source": [
    "## Dependencies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffcebeda-2f78-448c-a140-909dfefb7c17",
   "metadata": {},
   "source": [
    "**OPEN SOURCE PACKAGES**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4116446d-a634-4aac-92a0-46c91de00fad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install pdf2image #(includes poppler)\n",
    "# ! pip install -U langchain openai langchain-chroma langchain-experimental # (requires newest version for multi-modal)\n",
    "# ! pip install \"unstructured[all-docs]\" pillow pydantic lxml pillow matplotlib chromadb tiktoken\n",
    "# pip install -U langchain-openai"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbfe1183-9b62-4c5c-9a44-ff0656cd7f02",
   "metadata": {},
   "source": [
    "Additional packages may be required _(see installation instructions linked)_\n",
    "* [poppler](https://pdf2image.readthedocs.io/en/latest/installation.html) required for pdf handling\n",
    "* [tesseract](https://tesseract-ocr.github.io/tessdoc/Installation.html) required for OCR text recognition\n",
    "* [pytorch](https://pytorch.org/get-started/locally/#windows-installation) required when ingesting data via the [Unstructured](https://docs.unstructured.io/welcome) package"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6136471b-ccdd-48d2-bfd3-160dabfd9e9d",
   "metadata": {},
   "source": [
    "**LIBRARIES**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3434403e-3498-4014-a61c-9a8fc7c67e85",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Failed to import transformers.models.vision_encoder_decoder.modeling_vision_encoder_decoder because of the following error (look up to see its traceback):\nmodule 'torch' has no attribute 'version'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "File \u001b[1;32m~\\anaconda3\\envs\\dev\\lib\\site-packages\\transformers\\utils\\import_utils.py:1603\u001b[0m, in \u001b[0;36m_LazyModule._get_module\u001b[1;34m(self, module_name)\u001b[0m\n\u001b[0;32m   1602\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1603\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mimportlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimport_module\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m.\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmodule_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;18;43m__name__\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1604\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\dev\\lib\\importlib\\__init__.py:126\u001b[0m, in \u001b[0;36mimport_module\u001b[1;34m(name, package)\u001b[0m\n\u001b[0;32m    125\u001b[0m         level \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m--> 126\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_bootstrap\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_gcd_import\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpackage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1050\u001b[0m, in \u001b[0;36m_gcd_import\u001b[1;34m(name, package, level)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1027\u001b[0m, in \u001b[0;36m_find_and_load\u001b[1;34m(name, import_)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1006\u001b[0m, in \u001b[0;36m_find_and_load_unlocked\u001b[1;34m(name, import_)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:688\u001b[0m, in \u001b[0;36m_load_unlocked\u001b[1;34m(spec)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap_external>:883\u001b[0m, in \u001b[0;36mexec_module\u001b[1;34m(self, module)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:241\u001b[0m, in \u001b[0;36m_call_with_frames_removed\u001b[1;34m(f, *args, **kwds)\u001b[0m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\dev\\lib\\site-packages\\transformers\\models\\vision_encoder_decoder\\modeling_vision_encoder_decoder.py:28\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodeling_outputs\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BaseModelOutput, Seq2SeqLMOutput\n\u001b[1;32m---> 28\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodeling_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m PreTrainedModel\n\u001b[0;32m     29\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m add_start_docstrings, add_start_docstrings_to_model_forward, logging, replace_return_docstrings\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\dev\\lib\\site-packages\\transformers\\modeling_utils.py:41\u001b[0m\n\u001b[0;32m     40\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m CrossEntropyLoss, Identity\n\u001b[1;32m---> 41\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcheckpoint\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m checkpoint\n\u001b[0;32m     43\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mactivations\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get_activation\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\dev\\lib\\site-packages\\torch\\utils\\checkpoint.py:24\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfx\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtraceback\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mfx_traceback\u001b[39;00m\n\u001b[1;32m---> 24\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_functorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_aot_autograd\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfunctional_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m is_fun\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_pytree\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tree_map\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\dev\\lib\\site-packages\\torch\\_functorch\\_aot_autograd\\functional_utils.py:25\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_python_dispatch\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     22\u001b[0m     is_traceable_wrapper_subclass,\n\u001b[0;32m     23\u001b[0m     transform_subclass,\n\u001b[0;32m     24\u001b[0m )\n\u001b[1;32m---> 25\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m config\n\u001b[0;32m     27\u001b[0m aot_joint_log \u001b[38;5;241m=\u001b[39m getArtifactLogger(\u001b[38;5;18m__name__\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maot_joint_graph\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\dev\\lib\\site-packages\\torch\\_functorch\\config.py:60\u001b[0m\n\u001b[0;32m     44\u001b[0m \u001b[38;5;66;03m# When AOTAutograd regenerates aliased graph outputs,\u001b[39;00m\n\u001b[0;32m     45\u001b[0m \u001b[38;5;66;03m# attempte to use functionalization's view-replay logic\u001b[39;00m\n\u001b[0;32m     46\u001b[0m \u001b[38;5;66;03m# before falling back to the autograd engine's view replay or as_strided.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     58\u001b[0m \u001b[38;5;66;03m# once XLA pin update works,\u001b[39;00m\n\u001b[0;32m     59\u001b[0m \u001b[38;5;66;03m# or default config to true and fix relevant bugs\u001b[39;00m\n\u001b[1;32m---> 60\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_inductor\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconfig\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m is_fbcode\n\u001b[0;32m     62\u001b[0m view_replay_for_aliased_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m is_fbcode()\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\dev\\lib\\site-packages\\torch\\_inductor\\config.py:42\u001b[0m\n\u001b[0;32m     40\u001b[0m \u001b[38;5;66;03m# codegen cpp wrapper code in an ABI compatible mode\u001b[39;00m\n\u001b[0;32m     41\u001b[0m abi_compatible \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m---> 42\u001b[0m     os\u001b[38;5;241m.\u001b[39menviron\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTORCHINDUCTOR_ABI_COMPATIBLE\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m1\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mis_fbcode\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m0\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m1\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     43\u001b[0m )\n\u001b[0;32m     45\u001b[0m c_shim_version \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39menviron\u001b[38;5;241m.\u001b[39mget(\n\u001b[0;32m     46\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTORCHINDUCTOR_C_SHIM_VERSION\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m1\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_fbcode() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m2\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     47\u001b[0m )\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\dev\\lib\\site-packages\\torch\\_inductor\\config.py:10\u001b[0m, in \u001b[0;36mis_fbcode\u001b[1;34m()\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mis_fbcode\u001b[39m():\n\u001b[1;32m---> 10\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mversion\u001b[49m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgit_version\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\dev\\lib\\site-packages\\torch\\__init__.py:2216\u001b[0m, in \u001b[0;36m__getattr__\u001b[1;34m(name)\u001b[0m\n\u001b[0;32m   2214\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m importlib\u001b[38;5;241m.\u001b[39mimport_module(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;18m__name__\u001b[39m)\n\u001b[1;32m-> 2216\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodule \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'torch' has no attribute 'version'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 11\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# Data Loading\u001b[39;00m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain_text_splitters\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m CharacterTextSplitter \u001b[38;5;66;03m# Needed for data loading, partitioning PDFs, tables, text, and images\u001b[39;00m\n\u001b[1;32m---> 11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01munstructured\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpartition\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpdf\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m partition_pdf \u001b[38;5;66;03m# Needed for partitioning PDFs, tables, text, and images\u001b[39;00m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# Text, Table and Image Summaries\u001b[39;00m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain_core\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01moutput_parsers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m StrOutputParser \u001b[38;5;66;03m# Needed for summary generation\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\dev\\lib\\site-packages\\unstructured\\partition\\pdf.py:56\u001b[0m\n\u001b[0;32m     45\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01munstructured\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpartition\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcommon\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     46\u001b[0m     document_to_element_list,\n\u001b[0;32m     47\u001b[0m     exactly_one,\n\u001b[0;32m     48\u001b[0m     ocr_data_to_elements,\n\u001b[0;32m     49\u001b[0m     spooled_to_bytes_io_if_needed,\n\u001b[0;32m     50\u001b[0m )\n\u001b[0;32m     51\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01munstructured\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpartition\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlang\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     52\u001b[0m     check_language_args,\n\u001b[0;32m     53\u001b[0m     prepare_languages_for_tesseract,\n\u001b[0;32m     54\u001b[0m     tesseract_to_paddle_language,\n\u001b[0;32m     55\u001b[0m )\n\u001b[1;32m---> 56\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01munstructured\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpartition\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpdf_image\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01manalysis\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbbox_visualisation\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     57\u001b[0m     AnalysisDrawer,\n\u001b[0;32m     58\u001b[0m     FinalLayoutDrawer,\n\u001b[0;32m     59\u001b[0m     OCRLayoutDrawer,\n\u001b[0;32m     60\u001b[0m     ODModelLayoutDrawer,\n\u001b[0;32m     61\u001b[0m     PdfminerLayoutDrawer,\n\u001b[0;32m     62\u001b[0m )\n\u001b[0;32m     63\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01munstructured\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpartition\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpdf_image\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01manalysis\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlayout_dump\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     64\u001b[0m     JsonLayoutDumper,\n\u001b[0;32m     65\u001b[0m     ObjectDetectionLayoutDumper,\n\u001b[0;32m     66\u001b[0m )\n\u001b[0;32m     67\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01munstructured\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpartition\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpdf_image\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mform_extraction\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m run_form_extraction\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\dev\\lib\\site-packages\\unstructured\\partition\\pdf_image\\analysis\\bbox_visualisation.py:16\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01munstructured_inference\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconstants\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ElementType\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01munstructured_inference\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01minference\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01melements\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TextRegion\n\u001b[1;32m---> 16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01munstructured_inference\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01minference\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlayout\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DocumentLayout\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01munstructured_inference\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01minference\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlayoutelement\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LayoutElement\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01munstructured\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdocuments\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01melements\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Element, Text\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\dev\\lib\\site-packages\\unstructured_inference\\inference\\layout.py:19\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01munstructured_inference\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01minference\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlayoutelement\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     16\u001b[0m     LayoutElement,\n\u001b[0;32m     17\u001b[0m )\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01munstructured_inference\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlogger\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m logger\n\u001b[1;32m---> 19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01munstructured_inference\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbase\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get_model\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01munstructured_inference\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01munstructuredmodel\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     21\u001b[0m     UnstructuredElementExtractionModel,\n\u001b[0;32m     22\u001b[0m     UnstructuredObjectDetectionModel,\n\u001b[0;32m     23\u001b[0m )\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01munstructured_inference\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mvisualize\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m draw_bbox\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\dev\\lib\\site-packages\\unstructured_inference\\models\\base.py:7\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Dict, Optional, Tuple, Type\n\u001b[1;32m----> 7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01munstructured_inference\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mchipper\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m MODEL_TYPES \u001b[38;5;28;01mas\u001b[39;00m CHIPPER_MODEL_TYPES\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01munstructured_inference\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mchipper\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m UnstructuredChipperModel\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01munstructured_inference\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdetectron2onnx\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m MODEL_TYPES \u001b[38;5;28;01mas\u001b[39;00m DETECTRON2_ONNX_MODEL_TYPES\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\dev\\lib\\site-packages\\unstructured_inference\\models\\chipper.py:13\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcv2\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m MatLike\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mPIL\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mImage\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Image\n\u001b[1;32m---> 13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DonutProcessor, VisionEncoderDecoderModel\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mgeneration\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlogits_process\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LogitsProcessor\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mgeneration\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mstopping_criteria\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m StoppingCriteria\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1075\u001b[0m, in \u001b[0;36m_handle_fromlist\u001b[1;34m(module, fromlist, import_, recursive)\u001b[0m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\dev\\lib\\site-packages\\transformers\\utils\\import_utils.py:1594\u001b[0m, in \u001b[0;36m_LazyModule.__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   1592\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_class_to_module\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[0;32m   1593\u001b[0m     module \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_module(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_class_to_module[name])\n\u001b[1;32m-> 1594\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mmodule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1595\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1596\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodule \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m has no attribute \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\dev\\lib\\site-packages\\transformers\\utils\\import_utils.py:1593\u001b[0m, in \u001b[0;36m_LazyModule.__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   1591\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_module(name)\n\u001b[0;32m   1592\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_class_to_module\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m-> 1593\u001b[0m     module \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_module\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_class_to_module\u001b[49m\u001b[43m[\u001b[49m\u001b[43mname\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1594\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(module, name)\n\u001b[0;32m   1595\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\dev\\lib\\site-packages\\transformers\\utils\\import_utils.py:1605\u001b[0m, in \u001b[0;36m_LazyModule._get_module\u001b[1;34m(self, module_name)\u001b[0m\n\u001b[0;32m   1603\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m importlib\u001b[38;5;241m.\u001b[39mimport_module(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m module_name, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)\n\u001b[0;32m   1604\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m-> 1605\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m   1606\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailed to import \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodule_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m because of the following error (look up to see its\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1607\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m traceback):\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1608\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Failed to import transformers.models.vision_encoder_decoder.modeling_vision_encoder_decoder because of the following error (look up to see its traceback):\nmodule 'torch' has no attribute 'version'"
     ]
    }
   ],
   "source": [
    "# Uses OpenAI LLM\n",
    "import os # Needed to access API key\n",
    "\n",
    "# LLM Chat\n",
    "from langchain.chat_models import ChatOpenAI # Needed for chat model functionality\n",
    "from langchain.prompts import ChatPromptTemplate # Needed for using LangChain prompt templates\n",
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "# Data Loading\n",
    "from langchain_text_splitters import CharacterTextSplitter # Needed for data loading, partitioning PDFs, tables, text, and images\n",
    "from unstructured.partition.pdf import partition_pdf # Needed for partitioning PDFs, tables, text, and images\n",
    "\n",
    "# Text, Table and Image Summaries\n",
    "from langchain_core.output_parsers import StrOutputParser # Needed for summary generation\n",
    "import base64\n",
    "\n",
    "# Vectorstores\n",
    "import uuid # Needed for generating document ids\n",
    "from langchain.retrievers.multi_vector import MultiVectorRetriever # Needed to perform vector retrieval\n",
    "from langchain.storage import InMemoryStore # Need to support proper content storage\n",
    "from langchain_chroma import Chroma # Needed for accessing vector database\n",
    "from langchain_core.documents import Document # Needed to \n",
    "from langchain_openai import OpenAIEmbeddings # Needed to generate vector embeddings\n",
    "\n",
    "# RAG Retrieval\n",
    "import io\n",
    "import re\n",
    "from IPython.display import HTML, display\n",
    "from langchain_core.runnables import RunnableLambda, RunnablePassthrough\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6a60b65-5013-455e-a4c9-fe6ad89e9631",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function for loading API key\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "_ = load_dotenv(find_dotenv()) # reads local .env file\n",
    "\n",
    "openai.api_key = os.environ['OPENAI_API_KEY']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2b63509-2e34-4a98-b017-eedd04d19b36",
   "metadata": {},
   "source": [
    "## Setting Up LLM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "081e5211-27bd-4c75-a74d-d3dd0ef1dd78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OpenAI model selection\n",
    "llm_model = \"gpt-4o-mini\"\n",
    "\n",
    "# Uses temperature = 0.0 to reduce randomness in retrieved responses\n",
    "chat = ChatOpenAI(temperature=0.0, model=llm_model)\n",
    "\n",
    "# Optional - shows custom chat settings\n",
    "chat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7009a63d-40da-4ed1-8f28-b16d1c8acebe",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "outputs": [],
   "source": [
    "## Document Loading Capabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e31dadd-3a75-4b18-a69c-0ca7433a938a",
   "metadata": {},
   "source": [
    "**Partitioning Functions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d43bad8-cf75-4b0d-b478-015acebc2c71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracts elements from PDF\n",
    "def extract_pdf_elements(path, fname):\n",
    "    \"\"\"\n",
    "    Extract images, tables, and chunk text from a PDF file.\n",
    "    path: File path, which is used to dump images (.jpg)\n",
    "    fname: File name\n",
    "    \"\"\"\n",
    "    return partition_pdf(\n",
    "        filename=path + fname,\n",
    "        extract_images_in_pdf=False,\n",
    "        infer_table_structure=True,\n",
    "        chunking_strategy=\"by_title\",\n",
    "        max_characters=4000,\n",
    "        new_after_n_chars=3800,\n",
    "        combine_text_under_n_chars=2000,\n",
    "        image_output_dir_path=path,\n",
    "    )\n",
    "\n",
    "\n",
    "# Categorizes elements by type\n",
    "def categorize_elements(raw_pdf_elements):\n",
    "    \"\"\"\n",
    "    Categorize extracted elements from a PDF into tables and texts.\n",
    "    raw_pdf_elements: List of unstructured.documents.elements\n",
    "    \"\"\"\n",
    "    tables = []\n",
    "    texts = []\n",
    "    for element in raw_pdf_elements:\n",
    "        if \"unstructured.documents.elements.Table\" in str(type(element)):\n",
    "            tables.append(str(element))\n",
    "        elif \"unstructured.documents.elements.CompositeElement\" in str(type(element)):\n",
    "            texts.append(str(element))\n",
    "    return texts, tables\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ecb7326-0732-436a-ac44-1131c15fa090",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accesses documents\n",
    "# File path\n",
    "fpath = \"/Users/rlm/Desktop/cj/\"\n",
    "fname = \"cj.pdf\"\n",
    "\n",
    "# Get elements\n",
    "raw_pdf_elements = extract_pdf_elements(fpath, fname)\n",
    "\n",
    "# Get text, tables\n",
    "texts, tables = categorize_elements(raw_pdf_elements)\n",
    "\n",
    "# Optional: Enforce a specific token size for texts\n",
    "text_splitter = CharacterTextSplitter.from_tiktoken_encoder(\n",
    "    chunk_size=4000, chunk_overlap=0\n",
    ")\n",
    "joined_texts = \" \".join(texts)\n",
    "texts_4k_token = text_splitter.split_text(joined_texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7456af2c-ce59-4e0d-834a-6a21cc3f4f3f",
   "metadata": {},
   "source": [
    "## Document Processing Capbilities\n",
    "Uses multi-vector-retriever to index image (and / or text, table) summaries,\\\n",
    "but retrieve raw images (along with raw texts or tables).\\\n",
    "Sourced from LangChain Cookbook (_https://github.com/langchain-ai/langchain/blob/master/cookbook/Multi_modal_RAG.ipynb_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fadf3a3-7c30-4fc5-8410-f37d1899e007",
   "metadata": {},
   "source": [
    "**Text and Table Summary Function**\\\n",
    "Uses GPT-4 to produce table and text summaries used to retrieve raw tables and raw chunks of text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd40bc94-a32c-43ea-ba0e-016897e5e533",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate summaries of text elements\n",
    "def generate_text_summaries(texts, tables, summarize_texts=False):\n",
    "    \"\"\"\n",
    "    Summarize text elements\n",
    "    texts: List of str\n",
    "    tables: List of str\n",
    "    summarize_texts: Bool to summarize texts\n",
    "    \"\"\"\n",
    "\n",
    "    # Prompt\n",
    "    prompt_text = \"\"\"You are an assistant tasked with summarizing tables and text for retrieval. \\\n",
    "    These summaries will be embedded and used to retrieve the raw text or table elements. \\\n",
    "    Give a concise summary of the table or text that is well optimized for retrieval. Table or text: {element} \"\"\"\n",
    "    prompt = ChatPromptTemplate.from_template(prompt_text)\n",
    "\n",
    "    # Text summary chain\n",
    "    model = ChatOpenAI(temperature=0, model=\"gpt-4\")\n",
    "    summarize_chain = {\"element\": lambda x: x} | prompt | model | StrOutputParser()\n",
    "\n",
    "    # Initialize empty summaries\n",
    "    text_summaries = []\n",
    "    table_summaries = []\n",
    "\n",
    "    # Apply to text if texts are provided and summarization is requested\n",
    "    if texts and summarize_texts:\n",
    "        text_summaries = summarize_chain.batch(texts, {\"max_concurrency\": 5})\n",
    "    elif texts:\n",
    "        text_summaries = texts\n",
    "\n",
    "    # Apply to tables if tables are provided\n",
    "    if tables:\n",
    "        table_summaries = summarize_chain.batch(tables, {\"max_concurrency\": 5})\n",
    "\n",
    "    return text_summaries, table_summaries\n",
    "\n",
    "\n",
    "# Get text, table summaries\n",
    "text_summaries, table_summaries = generate_text_summaries(\n",
    "    texts_4k_token, tables, summarize_texts=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09ed3d45-b800-4055-b529-3c220ebe3ce6",
   "metadata": {},
   "source": [
    "**Image Summary Function**\\\n",
    "Uses GPT-4V to produce the image summaries. See API documenation here: https://platform.openai.com/docs/guides/vision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb61a52a-dff5-4cd2-bafc-a171d58c08f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import base64\n",
    "\n",
    "\n",
    "# def encode_image(image_path):\n",
    "#     \"\"\"Getting the base64 string\"\"\"\n",
    "#     with open(image_path, \"rb\") as image_file:\n",
    "#         return base64.b64encode(image_file.read()).decode(\"utf-8\")\n",
    "\n",
    "\n",
    "# def image_summarize(img_base64, prompt):\n",
    "#     \"\"\"Make image summary\"\"\"\n",
    "#     chat = ChatOpenAI(model=\"gpt-4-vision-preview\", max_tokens=1024)\n",
    "\n",
    "#     msg = chat.invoke(\n",
    "#         [\n",
    "#             HumanMessage(\n",
    "#                 content=[\n",
    "#                     {\"type\": \"text\", \"text\": prompt},\n",
    "#                     {\n",
    "#                         \"type\": \"image_url\",\n",
    "#                         \"image_url\": {\"url\": f\"data:image/jpeg;base64,{img_base64}\"},\n",
    "#                     },\n",
    "#                 ]\n",
    "#             )\n",
    "#         ]\n",
    "#     )\n",
    "#     return msg.content\n",
    "\n",
    "\n",
    "# def generate_img_summaries(path):\n",
    "#     \"\"\"\n",
    "#     Generate summaries and base64 encoded strings for images\n",
    "#     path: Path to list of .jpg files extracted by Unstructured\n",
    "#     \"\"\"\n",
    "\n",
    "#     # Store base64 encoded images\n",
    "#     img_base64_list = []\n",
    "\n",
    "#     # Store image summaries\n",
    "#     image_summaries = []\n",
    "\n",
    "#     # Prompt\n",
    "#     prompt = \"\"\"You are an assistant tasked with summarizing images for retrieval. \\\n",
    "#     These summaries will be embedded and used to retrieve the raw image. \\\n",
    "#     Give a concise summary of the image that is well optimized for retrieval.\"\"\"\n",
    "\n",
    "#     # Apply to images\n",
    "#     for img_file in sorted(os.listdir(path)):\n",
    "#         if img_file.endswith(\".jpg\"):\n",
    "#             img_path = os.path.join(path, img_file)\n",
    "#             base64_image = encode_image(img_path)\n",
    "#             img_base64_list.append(base64_image)\n",
    "#             image_summaries.append(image_summarize(base64_image, prompt))\n",
    "\n",
    "#     return img_base64_list, image_summaries\n",
    "\n",
    "\n",
    "# # Image summaries\n",
    "# img_base64_list, image_summaries = generate_img_summaries(fpath)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40292e62-c91c-4d6f-aab8-ed2bcfdfc3f1",
   "metadata": {},
   "source": [
    "## Content Storage and Retrieval Capabilties\n",
    "Sourced from LangChain Cookbook (_https://github.com/langchain-ai/langchain/blob/master/cookbook/Multi_modal_RAG.ipynb_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1d9c9f1-634f-452d-af7e-e5c675555935",
   "metadata": {},
   "source": [
    "**Content Storage Function**\\\n",
    "Stores raw texts, tables, and images in the docstore; Stores texts, table and image summaries in the vectorstore for efficient semantic retrieval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4e03acf-6bb5-434f-b439-82d78b5560f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_multi_vector_retriever(\n",
    "    vectorstore, text_summaries, texts, table_summaries, tables, image_summaries, images\n",
    "):\n",
    "    \"\"\"\n",
    "    Create retriever that indexes summaries, but returns raw images or texts\n",
    "    \"\"\"\n",
    "\n",
    "    # Initialize the storage layer\n",
    "    store = InMemoryStore()\n",
    "    id_key = \"doc_id\"\n",
    "\n",
    "    # Create the multi-vector retriever\n",
    "    retriever = MultiVectorRetriever(\n",
    "        vectorstore=vectorstore,\n",
    "        docstore=store,\n",
    "        id_key=id_key,\n",
    "    )\n",
    "\n",
    "    # Helper function to add documents to the vectorstore and docstore\n",
    "    def add_documents(retriever, doc_summaries, doc_contents):\n",
    "        doc_ids = [str(uuid.uuid4()) for _ in doc_contents]\n",
    "        summary_docs = [\n",
    "            Document(page_content=s, metadata={id_key: doc_ids[i]})\n",
    "            for i, s in enumerate(doc_summaries)\n",
    "        ]\n",
    "        retriever.vectorstore.add_documents(summary_docs)\n",
    "        retriever.docstore.mset(list(zip(doc_ids, doc_contents)))\n",
    "\n",
    "    # Add texts, tables, and images\n",
    "    # Check that text_summaries is not empty before adding\n",
    "    if text_summaries:\n",
    "        add_documents(retriever, text_summaries, texts)\n",
    "    # Check that table_summaries is not empty before adding\n",
    "    if table_summaries:\n",
    "        add_documents(retriever, table_summaries, tables)\n",
    "    # Check that image_summaries is not empty before adding\n",
    "    if image_summaries:\n",
    "        add_documents(retriever, image_summaries, images)\n",
    "\n",
    "    return retriever\n",
    "\n",
    "\n",
    "# The vectorstore to use to index the summaries\n",
    "vectorstore = Chroma(\n",
    "    collection_name=\"mm_rag_cj_blog\", embedding_function=OpenAIEmbeddings()\n",
    ")\n",
    "\n",
    "# Create retriever\n",
    "retriever_multi_vector_img = create_multi_vector_retriever(\n",
    "    vectorstore,\n",
    "    text_summaries,\n",
    "    texts,\n",
    "    table_summaries,\n",
    "    tables,\n",
    "    image_summaries,\n",
    "    img_base64_list,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a9d08b3-b3e9-4c95-862e-9d0ed1b8c62f",
   "metadata": {},
   "source": [
    "**Content Retrieval Functions**\\\n",
    "Bins the retrieved doc(s) into the correct parts of the GPT-4V prompt template."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "292872a0-c3d5-42c0-a50f-b9ff8268c4ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def plt_img_base64(img_base64):\n",
    "    \"\"\"Disply base64 encoded string as image\"\"\"\n",
    "    # Create an HTML img tag with the base64 string as the source\n",
    "    image_html = f'<img src=\"data:image/jpeg;base64,{img_base64}\" />'\n",
    "    # Display the image by rendering the HTML\n",
    "    display(HTML(image_html))\n",
    "\n",
    "\n",
    "def looks_like_base64(sb):\n",
    "    \"\"\"Check if the string looks like base64\"\"\"\n",
    "    return re.match(\"^[A-Za-z0-9+/]+[=]{0,2}$\", sb) is not None\n",
    "\n",
    "\n",
    "def is_image_data(b64data):\n",
    "    \"\"\"\n",
    "    Check if the base64 data is an image by looking at the start of the data\n",
    "    \"\"\"\n",
    "    image_signatures = {\n",
    "        b\"\\xff\\xd8\\xff\": \"jpg\",\n",
    "        b\"\\x89\\x50\\x4e\\x47\\x0d\\x0a\\x1a\\x0a\": \"png\",\n",
    "        b\"\\x47\\x49\\x46\\x38\": \"gif\",\n",
    "        b\"\\x52\\x49\\x46\\x46\": \"webp\",\n",
    "    }\n",
    "    try:\n",
    "        header = base64.b64decode(b64data)[:8]  # Decode and get the first 8 bytes\n",
    "        for sig, format in image_signatures.items():\n",
    "            if header.startswith(sig):\n",
    "                return True\n",
    "        return False\n",
    "    except Exception:\n",
    "        return False\n",
    "\n",
    "\n",
    "def resize_base64_image(base64_string, size=(128, 128)):\n",
    "    \"\"\"\n",
    "    Resize an image encoded as a Base64 string\n",
    "    \"\"\"\n",
    "    # Decode the Base64 string\n",
    "    img_data = base64.b64decode(base64_string)\n",
    "    img = Image.open(io.BytesIO(img_data))\n",
    "\n",
    "    # Resize the image\n",
    "    resized_img = img.resize(size, Image.LANCZOS)\n",
    "\n",
    "    # Save the resized image to a bytes buffer\n",
    "    buffered = io.BytesIO()\n",
    "    resized_img.save(buffered, format=img.format)\n",
    "\n",
    "    # Encode the resized image to Base64\n",
    "    return base64.b64encode(buffered.getvalue()).decode(\"utf-8\")\n",
    "\n",
    "\n",
    "def split_image_text_types(docs):\n",
    "    \"\"\"\n",
    "    Split base64-encoded images and texts\n",
    "    \"\"\"\n",
    "    b64_images = []\n",
    "    texts = []\n",
    "    for doc in docs:\n",
    "        # Check if the document is of type Document and extract page_content if so\n",
    "        if isinstance(doc, Document):\n",
    "            doc = doc.page_content\n",
    "        if looks_like_base64(doc) and is_image_data(doc):\n",
    "            doc = resize_base64_image(doc, size=(1300, 600))\n",
    "            b64_images.append(doc)\n",
    "        else:\n",
    "            texts.append(doc)\n",
    "    return {\"images\": b64_images, \"texts\": texts}\n",
    "\n",
    "\n",
    "def img_prompt_func(data_dict):\n",
    "    \"\"\"\n",
    "    Join the context into a single string\n",
    "    \"\"\"\n",
    "    formatted_texts = \"\\n\".join(data_dict[\"context\"][\"texts\"])\n",
    "    messages = []\n",
    "\n",
    "    # Adding image(s) to the messages if present\n",
    "    if data_dict[\"context\"][\"images\"]:\n",
    "        for image in data_dict[\"context\"][\"images\"]:\n",
    "            image_message = {\n",
    "                \"type\": \"image_url\",\n",
    "                \"image_url\": {\"url\": f\"data:image/jpeg;base64,{image}\"},\n",
    "            }\n",
    "            messages.append(image_message)\n",
    "\n",
    "    # Adding the text for analysis\n",
    "    text_message = {\n",
    "        \"type\": \"text\",\n",
    "        \"text\": (\n",
    "            \"You are financial analyst tasking with providing investment advice.\\n\"\n",
    "            \"You will be given a mixed of text, tables, and image(s) usually of charts or graphs.\\n\"\n",
    "            \"Use this information to provide investment advice related to the user question. \\n\"\n",
    "            f\"User-provided question: {data_dict['question']}\\n\\n\"\n",
    "            \"Text and / or tables:\\n\"\n",
    "            f\"{formatted_texts}\"\n",
    "        ),\n",
    "    }\n",
    "    messages.append(text_message)\n",
    "    return [HumanMessage(content=messages)]\n",
    "\n",
    "\n",
    "def multi_modal_rag_chain(retriever):\n",
    "    \"\"\"\n",
    "    Multi-modal RAG chain\n",
    "    \"\"\"\n",
    "\n",
    "    # Multi-modal LLM\n",
    "    model = ChatOpenAI(temperature=0, model=\"gpt-4-vision-preview\", max_tokens=1024)\n",
    "\n",
    "    # RAG pipeline\n",
    "    chain = (\n",
    "        {\n",
    "            \"context\": retriever | RunnableLambda(split_image_text_types),\n",
    "            \"question\": RunnablePassthrough(),\n",
    "        }\n",
    "        | RunnableLambda(img_prompt_func)\n",
    "        | model\n",
    "        | StrOutputParser()\n",
    "    )\n",
    "\n",
    "    return chain\n",
    "\n",
    "\n",
    "# Create RAG chain\n",
    "chain_multimodal_rag = multi_modal_rag_chain(retriever_multi_vector_img)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afc4fc1d-d1a6-4bb6-8f9e-7520f7e37c76",
   "metadata": {},
   "source": [
    "# USING RMULTIMODAL RAG CAPABILITY"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83bd08cd-5914-4af8-838f-875194a975f2",
   "metadata": {},
   "source": [
    "**Retrieval Example**\\\n",
    "Examines retrieval to check that images are relevant to query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "348ee2e7-0c82-443c-a708-44f7cc0d075a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check retrieval\n",
    "query = \"Give me company names that are interesting investments based on EV / NTM and NTM rev growth. Consider EV / NTM multiples vs historical?\"\n",
    "docs = retriever_multi_vector_img.invoke(query, limit=6)\n",
    "\n",
    "# We get 4 docs\n",
    "len(docs)\n",
    "\n",
    "# Check retrieval\n",
    "query = \"What are the EV / NTM and NTM rev growth for MongoDB, Cloudflare, and Datadog?\"\n",
    "docs = retriever_multi_vector_img.invoke(query, limit=6)\n",
    "\n",
    "# Checks number of documents returned\n",
    "len(docs)\n",
    "\n",
    "# # Returns relevant images and image summary\n",
    "# plt_img_base64(docs[0])\n",
    "# image_summaries[3]\n",
    "\n",
    "# Runs RAG and test ability to synthesize an answer to question.\n",
    "chain_multimodal_rag.invoke(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79f3e837-3f2c-4e40-bc2f-8cd1f18f882f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6cb26e5-78ed-4e16-b9fb-1faa3b0f203c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dev",
   "language": "python",
   "name": "dev"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
